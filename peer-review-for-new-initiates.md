## Peer Review Should Serve Science

<!-- ##### Scientific peer review is systemically toxic, and will be, so long as scientists allow it to be co-opted for social stratification. -->

<!-- em — , en – -->

<!-- **Reviewing research to rank its perceived *importance* is incompatible with reviewing for *accuracy* and *objectivity*.** -->

It is a convenient myth that the primary purpose of peer review is to evaluate the accuracy and objectivity of scientific research to protect scientific integrity. Ensuring research does not mislead is, indeed, a noble purpose. No matter how expert scientists are in designing, conducting, and presenting their research, they should invite scrutiny by those who are disinterested but knowledgeable. Sufficiently knowledgeable "peers" may catch errors or misleading statements that authors' may have missed, in part because they have less invested in the outcome, but also because they are less subject to the group think that can evolve when scientists collaborate as authors.

However, the overriding purpose of peer review, as it is actually practiced in most of science, is *ranking*. Peer review is the linchpin of the system that we scientists use to rank the value of research and, by extension, to rank each other. We submit our research to conferences and journals each with has its own brand and tier of prestige. They each have their own peer review processes, reviewers, and standards for acceptance, all of which are chosen to signal that tier of prestige.

These conferences and journals are part of a larger social stratification system in which we are taught to participate. We rank the importance of research by where it is published. We rank the importance of scientists by their ability to accumulate publications with prestigious brands. We use these rankings to decide who is worthy of acclaim, jobs, trust, and attention.

This stratification system requires peer review committees to evaluate requirements beyond scientific integrity, lest too many papers would be included in the top strata. To create exclusivity, peer review committees add requirements such that research be *novel*, *interesting*, *impactful*, or that it meets the the particularly nebulous catch-all (or *drop-all* for rejecting any paper) of having a *significant contribution*.

<!-- Ranking conflicts with integrity -->

There are myriad ways in which these *ranking requirements* conflict with, and subvert, scientific-integrity requirements.

*Ranking requirements bias what gets published.* For example, submissions that confirm the hopes and beliefs of peer reviewers will be more likely deemed significant, and be accepted, than results that defy reviewers' hopes. If peer review committees deem null results uninteresting, and twenty research teams test an attractive-but-false hypothesis, we can expect 19 results to be unpublishable and the only paper to survive peer review and to report the incorrect result that this false hypothesis is true (with the null hypothesis rejected with probability $p<0.05$.)

*Ranking requirements bias how researchers write their papers.* To make papers more more publishable, authors may feel compelled to make them less scientific. Researchers may not detail their experimental designs in sufficient detail for results to be replicated, as such mundane information may be deemed uninteresting by reviewers. Better to have more space to explain why the results are significant. Also, authors are compelled to highlight, and dedicate more space to, experiments or tests that yielded a statistically significant or novel result.

*Ranking requirements can be toxic to the ideals of scientists performing research* and trying to publish it. Instead of focusing on scientific integrity, we are forced to focus on meeting subjective expectations of what the peers in our community expect a paper to look like. Those of us who would prefer to leave the interpretation of results and speculation to others are compelled by norms to add subjective "discussion" sections tainted by our perspectives. When writing on a new topic that may only have five or ten prior papers relevant enough to cite, we are compelled to add dozens more meet the ever-escalating average citation counts that reviewers expect, even if we believe it would be better to dedicate more space to the truly relevant prior work. We invest time and effort we could be spending furthering science crafting our papers to meet ranking requirements, then are forced to delay publication and revise our papers to meet the subjective expectations of the next set of reviewers based on the subjective feedback of the previous reviewers. And, of course, there is the mental health impact of being expected to participate in a system that requires that the outcome of most submissions will be rejection, and in which reviewers will inevitably feel obligated to provide feedback that justifies their decision to reject – feedback that will inevitably focus on shortcomings – to their fellow committee members, to themselves, and to the authors.

*Ranking requirements also toxic to reviewers.* We are burdened by the work of reviewing papers that had already met scientific integrity requirements when previously rejected. Since the game we play encourages authors to re-submit often, efforts to provide constructive may be wasted. We feel obligated to participate in peer review because of its importance for protecting scientific integrity, but it is nearly impossible to do so without becoming complicit in the toxicity of the social stratification system built upon it. We are obligated to work with toxic reviewers who abuse the subjectivity of peer review, and to protect their identities, while observing damage they inflict that causes aspiring scientists to abandon careers in science.

<!-- The lies we tell ourselves -->

One lie that allows us to perpetuate the stratification system is that responsibility for peer review's toxicity lies exclusively with the most egregious reviewers. The truth is that they are merely the most visible symptoms of the very cancer we ourselves are feeding.

Another illusion that props up stratification is that there is value in using peer review to "curate" research to separate the *important* work others should take notice of from from that which is merely *accurate*. Using peer review for curation may have had value before papers could be shared for free online, before the invention of myriad tools to filter the resulting onslaught of information, and before the introduction of remote presentations that scale to accommodate any number of people who want to participate. Peer review no longer needs to serve this purpose. Ironically, those of us in Computer Science, whose technical contributions have disrupts other industries, are disturbingly averse to changing with the times.

Lastly, we may believe that there is simply no escape from using peer review for stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

<!-- Ranking conflicts with integrity -->

Yet, there are plenty of opportunities outside of peer review to support stratification systems for those that need them.


To those who argue that peer review needs to be selective to curate which research should be presented, and which should, there are obvious alternatives that do not require an end to in-person conferences.

<!-- We do not need ranking to determine whether a research presentation deserves a large in-person audience, a small audience, or no in-person audience. -->

<!-- To Escaping this system that co-opts peer review for ranking requires that we come to terms with our past complicity and commit to alternatives. -->

<!-- s
 - authors expend time writing and revising papers, and space in papers, to increasing perceived importance
 - work is rejected and publication delayed due to lack of perceived importance.
 - reviewing workloads increase as each peer review process necessitates a new set of reviewers to determine if the work meets that publications' threshold of importance
  -->

<!-- View science as a system. Guarded by toxicity. Only those who are immune the the toxicity enter. They become the new guards.
"This is how we've always done it".
Peer review for ranking is endemic to science, it is also toxic to science.
 - Once can't participate in peer review without being complicit in ranking
 - Biases science.
 - We have biased science to include only those comfortable with their complicity in this system. -->



<!-- 
It ensures we have no idea how many experiments were run and discarded because results were insignificant, and so we cannot gauge whether our thresholds of statistical significance are meaningful.   
When scientists need to present that work as *important*, not merely correct, they will inevitably be tempted to subconciously or conciously mislead others.  exclude presenting results that appear unimportant (those with statistically insignificant results), find ways to make results appear more important, or just keep re-submitting their work to different venues until reaching sympathetic reviewers who agree that it is important. Those unwilling to take such measures and tough out the mental health consequences of frequent rejection will be filtered out of scientific careers. -->



<!-- Most every scientist who has submitted work for peer review has received ill-informed, aggressive, or otherwise toxic reviews.

The most comforting explanation for the toxicity that pervades peer review is that the responsible reviewers are a small but egregiously toxic subset that poison the process—the so-called *Reviewer B*s. Alas, it cannot explain how it is possible that most papers submitted for review are rejected despite being co-authored by career scientists who have successfully conducted and published research before.  -->

<!-- Most explanations for the toxicity endemic to scientific peer review conveniently put the blame on to *others*—the *Reviewer B*s that seem to pervasive in our reviews but who cannot be our friends or ourselves. -->

#### Social ranking is harmful to peer review


By perpetuating the co-option of peer review for social ranking, we are responsible for making science less objective and accurate. We create incentives for scientists to report only those tests that surpass a threshold of statistical significance, and not to not report those that fail to reach statistical significance, thereby exaggerating the implied likelihood of those that are reported. We discourage scientists from disclosing reasons why we might not want to trust their results, and we reward those who exaggerate results or contributions.

When peer review committees expect submitted papers to be of a quality that feels subjectively as good as those previously published, they end up rejecting diverse work for simply being different. Thus, scientists expend a great amount of time and effort to meet expectations that have nothing to do with science. For example, most papers in my field typically reference 50-100 citations. Those writing a paper on an emerging topic may only need to cite five or ten citations, but will often add dozens of tangentially-related works to ensure their citation count is at least above the average of past accepted papers and that no reviewer will feel slighted by not having their work cited. The result is that the number of citations grows each year, most research papers have far more citations than they actually need, and so authors no longer have space to actually provide background and insight into the prior work that is actually important for the reader to understand.

The subjectivity of social ranking gives aspiring scientists toxic mixed messages.  We reject intuitive results from being unsurprising or insufficiently novel, then reject results we find too surprising because such counterintuitive findings should not be published without further evidence. We fetishize novelty, encouraging new researchers to chase ideas that others identified as flawed before pursuing. When a novel approach succeeds, we may reject it because we can imagine a less-novel approach that we intuit to be superior that, to meet our subjective definition of a *significant contribution*, should have been tried as well.

Any reviewer self-confident enough to believe that they could have done a better job of designing, conducting, or presenting another's research can reject it for choices they didn't approve of.

When forced to justify why papers failed to meet our subjective criteria, we are more likely to focus on criticizing the work than on encouraging the changes that would make it reach our standards of objectivity and accuracy. To justify rejection on subjective criteria, the feedback is often itself nebulous (e.g., "the authors should do more to clarify their contribution.") Toxicity is inevitable when the primary purpose of feedback is to justify a decision and our social stratification system demands that most decisions are rejection.


Some advocates for co-opting peer review for subjective purposes believe that senior scientists should *curate* research so that others know what to consume. Aside from the questionable premise that senior scientists can see the future better than junior scientists, making peer review the mechanism for gatekeeping does not help junior scientists avoid investing time into ideas their elders are skeptical of---it only hinders their ability to share the outcome of that investment.

Adding myriad arbitrary justifications for rejecting scientific works from publication, and thereby delaying the dissemination of their findings, impedes scientific progress. It also forces us all to spend more time reviewing previously-rejected papers, and revising our own papers to meet the subjective whims of a previous reviewers (often only to have future reviewers objective to those changes.)

Lastly, but perhaps most importantly, a system that requires publications to be exclusive ensures that most of the feedback scientists will receive is that their work is not good enough. This is harmful to their mental health, and ensures that only those without sufficiently "thick skins" will survive as scientists.

#### Isolating scientific peer review from social ranking

Is the design sound. (Before conducting.)

Was it competently conducted and are the results presented accurately (after).  May not necessarily include conclusions beyond what is immediately clear from the paper.

Are the results worthy of attention? Where does it deserve to be presented? Does it deserve recognition?

We must create different systems for scientific peer review and for social ranking.

In a world with finite money, attention, and other resources, scientists will always need to compete in what may seem like zero-sum games.
When there is a conflict of interest, isolate it (don't ignore it).

Why must we make a decision on what is worthy of publication after an experiment is conducted, as to opposed to when it is designed?

In a world where information is effectively instant and free, and presentations can be in person or remote, why should program committees decide which research deserves a 1000-seat auditorium and which deserves to be available only online?



### How to separate

Curation will always be subjective, but learning that others are less interested in the questions you are investigating is less harmful than being rejected from a process with the implication that you have failed in other ways.

"Curation" is easy to add later. Journals that invite reviewed papers.  Conferences that distribute presentation space based on interest.

Liberates us to put review before running studies.

<!-- It will be impossible to participate in reviewing scientific work without being complicit in the mental health harms to scientists, and would-be scientists, of social stratification. -->


<!-- Sadly, this makes it impossible to participate in a process supposedly key to scientific objectivity without becoming complicit in the mental health harms to scientists and would-be scientists .  -->

<!-- The zero-sum game of distributing prestige is toxic to scientific objectivity and to the mental health of scientists and would-be scientists. When integrated into  and is a cancer on science itself.  Cue the offspring. -->

<!-- ### Why status ranking is inherently toxic

To make a publication selective, conferences and journals must determine whether research papers constitute a "significant contribution" worthy of the status of being associated with their brand.

The ambiguity in what "contribution" even means opens the floodgates of subjectivity, giving reviewers a blank check to reject work because they weren't sufficiently convinced it was important, or because it didn't cite work that they thought was worth mentioning, or because the way was organized in a way that they wouldn't have organized it. Anyone self-confident enough to believe that they could have done a better job of designing, conducting, or presenting the research can reject you for not doing what they would have done. -->

<!-- Destroy trust in the review process -->

When authors submit to a review process that uses subjective metrics, rather than scientific objectivity, they will optimize for those metrics. This reduced integrity, as some authors will try to inflate their importance of their findings or outright cheat. It also wastes a lot of time, as authors will take a paper that only builds on five or ten other prior papers worth citing, but will then add 50 or 100 citations so as not to have a below-average citation count and to ensure they don't risk citing work by potential reviewers who might fault the paper for not being deferential to their past contributions. Writing papers defensively in anticipation of subjective reviewing can be exhausting, especially when one failure to appease a reviewer's ego can set your publication date back by months or years.

But the real cost is to mental health.





<!-- As a member of a peer review committee, can dismiss a paper by arguing that authors should do a better job justifying the importance of the question they are investigating, or because the authors weren't sufficiently deferential to prior research, or because they didn't structure to put sections an order that reviewers prefer. -->

### Curation


#### Biasing participation to exclude

As a new graduate student, learning to recognize the abusiveness of this practice is key to surviving it and, hopefully, refusing to join those who accept as necessary (or at least inevitable).

You don't have the least power, and the system is designed to protect those with power over you.
Those who have power 
Your paper may be rejected because reviewers who compete with your advisor don't like the topic your advisors suggested you work on, or because you failed to cite their work, or because they didn't like.  . 

# Why is peer review abusive

Evaluate science to ensure that results don't mislead. (Both )

Determine whether research is novel, interesting, or otherwise significant enough to warrant publication at the venue it is being evaluated for.

<!-- Ranking the subjective value of creative work, and those who create it, is incompatible with the scientific process. -->

# It's not okay

Everyone thinks that the review process is random and that feedback is often unconstructive, and often abusive.  Everyone seems to have stories of reviewers who didn't read papers, who intentionally kept competing work out, or who rejected work because the reviewers' favorite work (often their own) wasn't cited.
Reviewer 2 memes.

Yet, nobody thinks they or their are that reviewer, even though many of us frequently confess to delaying reviews until the last minute when we have no choice but to rush them.  The number of peers we think of as abusive or careless and the number of abusive and careless reviews are inconsistent.

Our continued faith in the ritual of peer ritual, despite all the evidence we see as we and our students suffer its abuses, can feel cultish to the new initiate to our tribe.  The best time to prepare for this reality is now, before you establish your loyalty to the tribe, so that you can recognize how abusive it is and resolve not to carry that abuse forward.

# Biases participation in science

You have to be "thick skinned" enough to survive as a scientist despite this abusive process.
You have to be okay with participating in, and thus being complicit in the process.
Values protecting the anonymity of people with more power than you (reviewers) over your mental health.  (Anonymity is designed to protect junior faculty, but students can only aspire to have the power of those who get to subjectively evaluate the importance of their work anonymously and without consequences for errors.)


# 

# Compromises scientific integrity

# Delays the release of information


# Alternatives

Split these two purposes.

Even better, evaluate experiments before they are conducted!


<!--

What you can do:

Learning to recognize toxic behavior and rituals helps you to prevent yourself from being blinded by it, and becoming complicit in it.  Acknowledging the problem is a huge step!


-->
