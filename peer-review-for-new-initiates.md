## Rejecting Peer Review for Ranking

It is a convenient myth that the primary purpose of peer review is one of scientific integrity: to ensure research informs without misleading. Indeed, ensuring presented clearly, completely, objectively, and accurately *should* be paramount to scientists.

Alas, the overriding purpose of peer review, as it is actually practiced in most of science, is *ranking*.

Peer review is the linchpin of the system that we scientists use to rank the value of research and, by extension, to rank each other. We submit our research to conferences and journals each with its own brand and tier of prestige. Each conference and journals has its own peer review processes, reviewers, and standards for acceptance, all of which are chosen to signal its tier of prestige.

These conferences and journals are part of science's larger social stratification system: we rank the importance of research by where it is published; we rank the importance of scientists by their ability to accumulate publications with prestigious brands; we use these rankings to decide who is worthy of acclaim, jobs, trust, and attention.

This system requires reviewers to reject papers that merely satisfy scientific-integrity requirements, lest the top strata would be insufficiently exclusive. Review committees add requirements such that submitted research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

<!-- Ranking conflicts with integrity -->
---

Not only does ranking render scientific integrity secondary, *ranking requirements* conflict with, and subvert, scientific integrity.

*Ranking requirements bias what gets published.* Submissions that confirm the hopes and beliefs of peer reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Consider, for example, the consequences when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to to find no significant difference ($p>0.05$) and fail to publish. The only paper to publishable paper will claim to prove that attractive-but-false hypothesis that the intervention is effective (with the null hypothesis rejected with probability $p<0.05$).

*Ranking requirements encourage writing that is less scientific.* When documenting experimental designs in sufficient detail for replication makes a paper less appealing to reviewers who demand that it be interesting, authors may leave out important but mundane details. Authors feel compelled to highlight, and dedicate more space to, experiments or tests that yielded a statistically significant or novel result than those that are less "interesting".

*Ranking requirements can be toxic to the ideals of scientists performing research* and trying to publish it. We are craft papers to meet reviewers expectations of what an acceptable paper should like like, rather than what we think is best for readers. Those of us who would prefer to leave the interpretation of results and speculation to others are compelled to add subjective "discussion" sections, despite regretting that these cannot help but be tainted by our perspectives and make the writing less objective. When writing on a new topic that may only have five or ten prior papers relevant enough to cite, we are compelled to add dozens more meet the ever-escalating average citation counts that reviewers expect. We regretfully do this despite believing readers would be better served if we used that space to provide more insight into the smaller set of truly relevant prior work. We lament that papers have evolved to contain ever longer chains of citation numbers, most of which are present only to reduce our chance of rejection, but feel powerless to stop the trend.

*The process of submitting and re-submitting work to new reviewers wastes our time and theirs, and delays publication.* Time we could spend furthering science is lot to crafting and re-crafting our papers to meet ranking requirements. Work is delayed as revise our papers to meet the subjective expectations of the next set of reviewers based on the subjective feedback of the previous reviewers, then wait for the next submission deadline and response period.

---

Peer review is systemically toxic, and will be, so long as scientists allow it to be co-opted for ranking.

In rejecting papers, reviewers feel obligated to provide feedback that justifies the outcome to their fellow committee members, to themselves, and to the authors—feedback that must inevitably focus on shortcomings to justify rejection. A system that must reject the majority of papers, and provide feedback focused on justifying rejection, is harmful to the mental health of the community receiving far more negative feedback than positive feedback.

Ranking requirements also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific integrity requirements when previously rejected. Since our ranked peer review systems encourage authors to re-submit work rejected by one peer review committee to other peer review committees, authors may discard constructive feedback we may put care into writing. We feel obligated to participate in peer review because of its importance for protecting scientific integrity, but it is nearly impossible to do so without becoming complicit in the toxicity of the social stratification system built upon it. We are obligated to work with reviewers who abuse the subjectivity and anonymity of peer review, and obligated to protect their identities, while witnessing the damage they inflict that causes aspiring scientists to abandon careers in science.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to survive ranking-focused peer review explain our reasons for perpetuating this system ways using justifications eerily similar to the ones homeowners in exclusive neighborhoods use to fight opening them up. We talk about overcrowding our the loss we would feel if the traditions and character of our communities changed. We ignore the question of whether we are perpetuating a harmful system, and instead ask why we should have to give up that which we grew up with. We respond to criticisms of the system as attacks on ourselves, the friends, and colleagues (fellow reviewers, program chairs, and editors).

Ironically, the reason we are blind to the harms of ranking-focused peer review systems we are perpetuating is because we've all be part of the system so long that we lack an outsiders' objective perspective to see its flaws—we have no objective peers as we participate and our complicit in this system. And so, we've managed to collectively deceive ourselves into believing this is all okay.

We blame peer review's toxicity lies exclusively on the most egregious reviewers—a convenient other that is somehow among us despite not including any of our friends or colleagues. The truth is that we collectively reject far more work than a few toxic reviewers could achieve. The most toxic reviewers make great scapegoats, but they are merely the most visible symptoms of the very cancer we ourselves are feeding.

We also deceive ourselves by believing that, as reviewers, we provide value by helping to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable. No self-respecting social scientist would intentionally design a system this way.

Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does. Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times.

Lastly, we may excuse our complicity by believing that there is simply no escape from using peer review for ranking and stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

<!-- But we can -->
---

But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

We can start by creating new peer review systems that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness. If authors and one or more reviewers cannot agree on how to best improve a submitted work, or cannot agree on when or whether the work is ready to publish, reviewers' dissenting opinions could released in concert with, or event attached to, the published work. Such insight into the credibility of research is what the press and the public are looking for when they ask whether work has been peer reviewed.

Redesigning peer review to make science paramount also presents other opportunities. For human-subjects research, reviewing experimental designs prior to their execution could not only ensure reviewers are unbiased by the experiments' outcomes, but could allow methodological flaws identified by reviewers to be fixed before experiments are conducted.

Rethinking peer review could also allow us to rethink conferences, were papers are currently ranked to determine which are worthy of presentation slots. We could instead ask prospective attendees which of the scientifically-reviewed candidate presentations they would want to see and create a schedule optimized to give attendees what we want, rather than what reviewers guess we want, or what reviewers think we should want. Presentations with the most interest could be hosted in the largest lecture halls; presentations with a small but devoted audience could appear in more intimate spaces; papers with insufficient interest could be offered online or recorded. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

And once we have removed ranking from scientific peer review, the community of reviewers can grow to include those of us who want to serve science but unwilling to do so if it makes us complicit in the toxicity of ranking-focused peer review.

---

For better or worse, removing ranking from scientific peer review, or even from scientific research, will not bring an end to ranking. The systems and organizations that rely on ranking-focused peer review for stratification will adapt and survive, as they have in the past, so long as there is demand for them. In fact, many of the professional societies that host our conferences and publish our journals already have committees dedicated exclusively to stratifying their membership into "[grades](https://awards.acm.org/advanced-member-grades)" that applies to members who who don't publish any research.

For aspiring scientists, learning to recognize the toxicity of stratification is key to surviving it and, hopefully, surviving to be part of the change.

Until we change how we conduct peer review, each successive generation fo scientists will exclude amazing talent that failed to be "thick-skinned" and perseverant enough to survive in a system that has prioritized ranking over scientific integrity. Each generation will inflict harm on the next for no better reason that that "we've always done it this way" and we don't know how to change. We need to look at the system we have perpetuated with objectivity and find the courage to reject it.

<!-- em — , en – -->
