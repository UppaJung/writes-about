## Rejecting Peer Review for Ranking

<!-- ##### Scientific peer review is systemically toxic, and will be, so long as scientists allow it to be co-opted for social stratification. -->

<!-- em — , en – -->

<!-- **Reviewing research to rank its perceived *importance* is incompatible with reviewing for *accuracy* and *objectivity*.** -->

It is a convenient myth that the primary purpose of peer review is to protect scientific integrity—to ensure research informs without misleading. Indeed, scientific integrity should be paramount. Outside review is essential to helping ensure research is presented clearly, completely, objectively, and accurately.

However, the overriding purpose of peer review, as it is actually practiced in most of science, is *ranking*. Scientific integrity is a distant second because peer review is the linchpin of the system that we scientists use to rank the value of research and, by extension, to rank each other. We submit our research to conferences and journals each with its own brand and tier of prestige. Each conference and journals has its own peer review processes, reviewers, and standards for acceptance, all of which are chosen to signal its tier of prestige.

These conferences and journals are part of science's larger social stratification system: We rank the importance of research by where it is published; We rank the importance of scientists by their ability to accumulate publications with prestigious brands; We use these rankings to decide who is worthy of acclaim, jobs, trust, and attention.

This stratification system requires peer review committees to reject papers that merely scientific-integrity requirements, lest the top strata would be insufficiently exclusive. They do so by adding requirements such that a research submission be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

<!-- Ranking conflicts with integrity -->
<!-- #### Peer review for science and ranking are incompatible -->
---

Not only does ranking render scientific integrity secondary, *ranking requirements* conflict with, and subvert, scientific integrity.

*Ranking requirements bias what gets published.* Submissions that confirm the hopes and beliefs of peer reviewers will be more likely deemed significant, and be accepted, than results that defy reviewers' expectations or dash their hopes. Consider, for example, the consequences when peer review committees deem null results insufficiently interesting or insufficiently impactful. If twenty research teams test an attractive-but-false hypothesis that an intervention will have an effect, we can expect 19 teams to to find no significant difference $p>0.05$ and be fail to publish their work. The only paper to survive peer review and to report the incorrect result that the attractive-but-false hypothesis is true (with the null hypothesis rejected with probability $p<0.05$.)

*Ranking requirements bias how researchers write their papers.* To make papers more more publishable, authors may feel compelled to make them less scientific. Researchers may not document their experimental designs in sufficient detail for results to be replicated, as such mundane information may be deemed tedious and uninteresting by reviewers. Authors are compelled to instead use that space to explain why the results are important. Also, authors are compelled to highlight, and dedicate more space to, experiments or tests that yielded a statistically significant or novel result than tests that did not do so.

*Ranking requirements can be toxic to the ideals of scientists performing research* and trying to publish it. Instead of focusing on scientific integrity, we are forced to focus on meeting subjective expectations of what the peers in our community expect a paper to look like. Those of us who would prefer to leave the interpretation of results and speculation to others are compelled to add subjective "discussion" sections, despite regretting that these cannot help but be tainted by our perspectives, because reviewers expect them. When writing on a new topic that may only have five or ten prior papers relevant enough to cite, we are compelled to add dozens more meet the ever-escalating average citation counts that reviewers expect. We regretfully do this despite believing readers would be better served if we used that space to provide more insight into the smaller set of truly relevant prior work. We lament that papers have evolved to contain ever longer chains of citation numbers, most of which are present only to reduce our chance of rejection, but feel powerless to stop the trend.

We invest time and effort we could be spending furthering science crafting our papers to meet ranking requirements, then are forced to delay publication and revise our papers to meet the subjective expectations of the next set of reviewers based on the subjective feedback of the previous reviewers.

In rejecting papers, reviewers feel obligated to provide feedback that justifies their decision to reject to their fellow committee members, to themselves, and to the authors—feedback that will inevitably focus on shortcomings. A system that must reject the majority of papers, and provide feedback focused on justifying rejection, is toxic to the mental health of the community receiving far more negative feedback than positive feedback. 

*Ranking requirements also toxic to reviewers.* We are burdened by the work of reviewing papers that had already met scientific integrity requirements when previously rejected. Since the game we play encourages authors to re-submit work to new reviewers, any efforts to provide constructive feedback that we might find value in a second reading is likely to be wasted. We feel obligated to participate in peer review because of its importance for protecting scientific integrity, but it is nearly impossible to do so without becoming complicit in the toxicity of the social stratification system built upon it. We are obligated to work with toxic reviewers who abuse the subjectivity of peer review, and to protect their identities, while observing damage they inflict that causes aspiring scientists to abandon careers in science.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

One lie that allows us to perpetuate the stratification system is that responsibility for peer review's toxicity lies exclusively with the most egregious reviewers. The truth is that they are merely the most visible symptoms of the very cancer we ourselves are feeding.

Another comforting illusion is that as reviewers, we provide value in helping to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable.

Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does. Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are ironically averse to changing with the times.

Lastly, we may believe that there is simply no escape from using peer review for stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

<!-- But we can -->
---

But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

We can start by creating new peer review systems strictly scoped to evaluate scientific integrity. Such committees wouldn't evaluate whether research is worth publishing, but would help to ensure that when research is published the end product is more objective, accurate, understandable, and complete than it would be without external review. If authors cannot agree on how to best improve a submitted work, or whether the work is ready to publish, reviewers' dissenting opinions could released in concert with, or event attached to, the published work. Such a process would give readers, whether  other scientists, the press, or public, objective insight into a work's credibility.

Re-thinking peer review to serve science could also have other benefits. For human-subjects research, reviewing experimental designs prior to their execution could help identify methodological flaws in time to fix them. It would also place the researchers' analysis plan  on record prior to data being collected.

---

For better or worse, removing ranking from scientific peer review, or even from scientific research, will not bring an end to systems that rank researchers. Many of the professional societies that host conferences and publish journals already have committees dedicated to stratifying their membership into "[grades](https://awards.acm.org/advanced-member-grades)", even for members who don't publish scientific research. The systems and organizations that rely on ranking-focused peer review for stratification will adapt and survive, as they have in the past, so long as there is demand for them.

"Elite" journals will continue to curate research they deem significant, accepting submissions that have already been scientifically peer reviewed or replicating the process. Many journals and conferences already have awards committees separate from the peer review process to identify notable papers, both new and old; they can either split scientific and ranking review into separate operations or leave scientific review to others. But, keeping scientific peer review is separate gives those of us who don't want to be complicit in the toxicity of ranking a chance to opt out.

Like journals, conferences could continue create committees to "curate" which research attendees get to see presented. Better would be better to ask prospective attendees which of the available papers  they want to see presented, and create a schedule optimized to give attendees what we want rather than what reviewers think we want. Presentations with the most interest could be hosted in the largest lecture halls. Presentations with a small but devoted audience could appear in more intimate spaces. Papers with insufficient interest could be offered online or recorded. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

---

Most every scientist who has submitted work for peer review has received ill-informed, aggressive, or otherwise toxic reviews. We all know there is a problem.

But for senior scientists, changing how we conduct peer review requires us to re-examine the systems that we grew up in, that we are now a part of perpetuating, and that our close friends and colleagues help to perpetuate. Ironically, we have been unable to recognize how flawed our process of peer review is because we lack the objective perspective to see this tragedy as outsiders would.

For aspiring scientists, learning to recognize the abusiveness of this practice is key to surviving it and, hopefully, refusing to join those who accept as necessary (or at least inevitable).

If we cannot change how we conduct peer review, each successive generation fo scientists will exclude those not sufficient "thick-skinned" and perseverant to face our perverse review practices, regardless of how talented they are as scientists. Each generation will inflict pain on the next for no better reason that that "we've always done it this way" and we don't know how to change.

