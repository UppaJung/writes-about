## Rejecting Peer Review for Ranking

<!-- ##### Scientific peer review is systemically toxic, and will be, so long as scientists allow it to be co-opted for social stratification. -->

<!-- em — , en – -->

<!-- **Reviewing research to rank its perceived *importance* is incompatible with reviewing for *accuracy* and *objectivity*.** -->

It is a convenient myth that the primary purpose of peer review is one of scientific integrity: to ensure research informs without misleading. Indeed, one would expect and hope that scientific goals would be paramount to scientists, and outside review helps ensure research is presented clearly, completely, objectively, and accurately.

However, the overriding purpose of peer review, as it is actually practiced in most of science, is *ranking*.

Scientific integrity is a distant second because peer review is the linchpin of the system that we scientists use to rank the value of research and, by extension, to rank each other. We submit our research to conferences and journals each with its own brand and tier of prestige. Each conference and journals has its own peer review processes, reviewers, and standards for acceptance, all of which are chosen to signal its tier of prestige.

These conferences and journals are part of science's larger social stratification system: we rank the importance of research by where it is published; we rank the importance of scientists by their ability to accumulate publications with prestigious brands; we use these rankings to decide who is worthy of acclaim, jobs, trust, and attention.

This stratification system requires peer review committees to reject papers that merely satisfy scientific-integrity requirements, lest the top strata would be insufficiently exclusive. Committees increase exclusivity by adding requirements such that submitted research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

<!-- Ranking conflicts with integrity -->
<!-- #### Peer review for science and ranking are incompatible -->
---

Not only does ranking render scientific integrity secondary, *ranking requirements* conflict with, and subvert, scientific integrity.

*Ranking requirements bias what gets published.* Submissions that confirm the hopes and beliefs of peer reviewers will be more likely deemed significant, and be accepted, than results that defy reviewers' expectations or dash their hopes. Consider, for example, the consequences when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to to find no significant difference ($p>0.05$) and fail to publish. The only paper to publishable paper will claim to prove that attractive-but-false hypothesis that the intervention is effective (with the null hypothesis rejected with probability $p<0.05$).

*Ranking requirements bias how researchers write their papers.* To make papers more more publishable, authors may feel compelled to make them less scientific. Researchers may not document their experimental designs in sufficient detail for results to be replicated, as such mundane information may be deemed tedious and uninteresting by reviewers. Authors may be compelled to instead use that space to explain why the results are important. Also, authors are compelled to highlight, and dedicate more space to, experiments or tests that yielded a statistically significant or novel result than those that are less "interesting".

*Ranking requirements can be toxic to the ideals of scientists performing research* and trying to publish it. Instead of focusing on scientific integrity, we are forced to focus on meeting subjective expectations of what the peers in our community expect a paper to look like. Those of us who would prefer to leave the interpretation of results and speculation to others are compelled to add subjective "discussion" sections, despite regretting that these cannot help but be tainted by our perspectives. When writing on a new topic that may only have five or ten prior papers relevant enough to cite, we are compelled to add dozens more meet the ever-escalating average citation counts that reviewers expect. We regretfully do this despite believing readers would be better served if we used that space to provide more insight into the smaller set of truly relevant prior work. We lament that papers have evolved to contain ever longer chains of citation numbers, most of which are present only to reduce our chance of rejection, but feel powerless to stop the trend.

We invest time and effort we could be spending furthering science crafting our papers to meet ranking requirements, then are forced to delay publication and revise our papers to meet the subjective expectations of the next set of reviewers based on the subjective feedback of the previous reviewers.

In rejecting papers, reviewers feel obligated to provide feedback that justifies the outcome to their fellow committee members, to themselves, and to the authors—feedback that must inevitably focus on shortcomings to justify rejection. A system that must reject the majority of papers, and provide feedback focused on justifying rejection, is toxic to the mental health of the community receiving far more negative feedback than positive feedback.

*Ranking requirements also toxic to reviewers.* We are burdened by the work of reviewing papers that had already met scientific integrity requirements when previously rejected. Since our ranked peer review systems encourage authors to re-submit work rejected by one peer review committee to other peer review committees, authors may discard constructive feedback we may put care into writing. We feel obligated to participate in peer review because of its importance for protecting scientific integrity, but it is nearly impossible to do so without becoming complicit in the toxicity of the social stratification system built upon it. We are obligated to work with toxic reviewers who abuse the subjectivity of peer review, and to protect their identities, while observing damage they inflict that causes aspiring scientists to abandon careers in science.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Those of us privileged enough to survive ranked peer review justify perpetuating this system ways eerily similar to homeowners in exclusive neighborhoods fighting affordable housing. We talk about overcrowding, our fears of seeing our community change, and of losing character and traditions. We ignore the question of whether we are perpetuating a system that serves science, and instead ask why we should have to give up a system that we have learned to thrive in. We feel that we, and our friends and colleagues who play important roles as program chairs and editors are being attacked. In short, we are ironically blind to the harm of ranking-based peer review because we lack an outsiders' objective perspective to see its flaws.

One self-deception that allows us to perpetuate the stratification system is that responsibility for peer review's toxicity lies exclusively with the most egregious reviewers. The truth is that we collectively reject far more work than a few toxic reviewers could achieve. The most toxic reviewers make great scapegoats, but they are merely the most visible symptoms of the very cancer we ourselves are feeding.

We also deceive ourselves by believing that, as reviewers, we provide value by helping to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable.

Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does. Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times.

Lastly, we may believe that there is simply no escape from using peer review for stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

<!-- But we can -->
---

But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

We can start by creating new peer review systems that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness. If authors and one or more reviewers cannot agree on how to best improve a submitted work, or cannot agree on when or whether the work is ready to publish, reviewers' dissenting opinions could released in concert with, or event attached to, the published work. Such insight into the scientific integrity and credibility of the research is what the press and the public are looking for when they ask whether it has been peer reviewed.

Re-thinking peer review to serve science could also have other benefits. For human-subjects research, reviewing experimental designs prior to their execution could help identify methodological flaws in time to fix them. It would also place the researchers' analysis plan  on record prior to data being collected.

And once we have separated peer review for scientific integrity for peer review for ranking, the community of reviewers can grow to include those of us averse to participating in raking and the toxicity inherent to it.

---

For better or worse, removing ranking from scientific peer review, or even from scientific research, will not bring an end to systems that rank researchers. The systems and organizations that rely on ranking-focused peer review for stratification will adapt and survive, as they have in the past, so long as there is demand for them. In fact, many of the professional societies that host our conferences and publish our journals already have committees dedicated exclusively to stratifying their membership – including those who don't publish any research – into "[grades](https://awards.acm.org/advanced-member-grades)".

"Elite" journals will continue to curate research to select that which they deem significant, either as they do now or by accepting submissions that have already been scientifically peer reviewed. Many journals and conferences already have awards committees separate from the peer review process to identify notable papers, both new and old; they can either split scientific and ranking review into separate operations or leave scientific review to others.

Like journals, conferences could continue to dedicate review committees to choose which research attendees will be presented. Better would be better to ask prospective attendees which of the candidate presentations they would want to see and create a schedule optimized to give attendees what we want, rather than what reviewers guess we want, or think we should want. Presentations with the most interest could be hosted in the largest lecture halls. Presentations with a small but devoted audience could appear in more intimate spaces. Papers with insufficient interest could be offered online or recorded. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

---

For aspiring scientists, learning to recognize the toxicity of ranked peer review is key to surviving it and, hopefully, surviving to be part of the change.

If we cannot change how we conduct peer review, each successive generation fo scientists will exclude amazing talent that failed to be "thick-skinned" and perseverant enough to survive in a system that has prioritized ranking over scientific integrity. Each generation will inflict harm on the next for no better reason that that "we've always done it this way" and we don't know how to change, or don't have the courage to.
