## Peer Review as we Say, Not as we Do

We scientists say we conduct peer review to protect scientific integrity: ensuring research is presented clearly, completely, objectively, and accurately. The press and public ask whether research is peer reviewed when gauging how much to trust it, and so preventing research from misleading *should* be the paramount goal. 

But what we do in much of science is co-opt peer review to rank the importance of research and, by extension, to rank each other. <!--
But, in much of science, protecting scientific integrity is not the predominant goal of peer review—in practice we use peer review to rank the importance of research and, by extension, to rank each other.
--> We publish our research through journals and conferences, each of which with its own brand, and each of which signals its strata of brand prestige by being selective in accepting submissions. In this stratification system, our ability to accumulate publications with exclusive brands determines our career trajectory and our share of acclaim, attention, and – yes – positions on prestigious peer review committees.

Peer review committees achieve exclusivity by rejecting many submissions that merely satisfy scientific-integrity requirements. To do so, they add requirements tailored to their stratification goals, such that research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*. 

There are many ways that this co-option of peer review for stratification conflicts with, and subverts, scientific integrity. And so, when we purport to conduct peer review to prevent scientists from unwittingly misleading ourselves and the public, and elide how we use it for stratification, we have already mislead ourselves and the public through this omission.

<!-- Ranking conflicts with integrity -->
---

One way that co-opting peer review for stratification causes science to mislead is by *biasing what gets published*. Submissions that confirm the hopes and beliefs of reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among the many consequences is the potential for false discovery, especially when when peer review committees deem null results insufficiently interesting to publish. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to find specious support the attractive-but-false hypothesis that the intervention is effective—and get published.

*Co-option undermines how we document our research.* When reviewers demand that papers be interesting and that writing flow well, authors elide mundane details of experimental designs that might bore reviewers, even if those details would be needed to replicate their experiments. Authors highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. Some authors deceive others, and even themselves, to believe that they had planned to conduct hypothesis tests prior to observing data when in fact their decision to conduct a test was influenced by differences they observed in the data.

*Co-options make papers less readable*, as reviewers expect papers to look like previously-accepted papers, whereas great scientific writing can take many forms that don't fit this mold. For example, reviewers often reject papers with fewer-than-average citations even if the topic has few works to cite. So authors often add dozens of unnecessary citations to ensure their count is above average, signal their expertise in the field, and reduce the chance that a reviewer will reject their work having failed to cite a reviewer's favorite work. It's no wonder papers have evolved to contain increasingly longer chains of citation numbers, with decreasing amounts context to assist the non-experts who actually benefit from citations to related work. If authors had the freedom to write papers to inform, rather than conform, we could use that space to provide more insight into the smaller set of truly relevant prior work.

*Co-option encourages subjective writing*, because competing for ranking encourages authors to aggrandize the impact of their research, and because many publications demand that authors go beyond factual reporting of results to speculate about impact.

*Structuring peer review as an admissions process delays the spread of knowledge*. Authors must often wait until specific deadlines to submit research, then give reviewers time to review enough submissions to rank them relative to each other and discuss papers in comparison to each other. Each submission typically takes months, and work must be re-submitted until accepted. Research that needs to be submitted three times to find a publication that considers it on brand may take over a year to be accepted and longer to publish.

*Structuring peer review as an admissions process disrupts scientists' work* by delaying feedback and requiring resubmissions. When we get feedback, months have often passed since we last worked on the paper and we must re-familiarize ourselves with what we wrote (and what we did that wasn't written). We still may not remember some details that reviewers inform us that we failed to document. Unless we are lucky enough to be accepted in the first round, we must re-submit to a different set of reviewers with different (often conflicting) expectations, paper formatting rules, and other customs. Each new submission adds months of delays that further impede our memories of our experiment, our findings, and the tooling we used to process our data. 

*The stratification system built on publication renders the "peer reviewed" valueless* as a signal of scientific integrity, because while high-strata publications have too many papers that meet scientific-integrity standards, lower-strata conferences and journals may need to publish sub-standard papers to meet financial or attendance goals. At the bottom strata, papers may only appear to be peer reviewed, receive no feedback from reviewers, and be published only on the condition that authors pay publication fees or conference-registration fees.

---

The way we conduct peer review for stratification also subverts science by biasing who becomes a scientist. Many aspiring scientists fail to survive the mostly negative feedback they receive from the community they are trying to join, even if they are more talented than some survivors who were more self confident or better able to promote the importance of their work. This talent will be discarded as too "thin-skinned" or "insufficiently perseverant".

And peer review co-opted for stratification cannot help but have mostly negative feedback. Peer review committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but the overriding feedback must still justify rejection if a paper is to be rejected. Any system that rejects a majority of papers will necessarily provide more negative feedback to the community than positive feedback. A system that provides mostly negative feedback is harmful to the mental health of those who rely upon it, especially to those most junior who have the most to lose and the least control over how the system works.

Peer review regimes built for stratification are also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific-integrity requirements when previously rejected. We write feedback to authors who may be content to send their work unchanged to the next set of reviewers. Experiments with known flaws, which should be published with disclosures of those flaws, are submitted over and over, to be reviewed over and over, until reaching reviewers willing to allow it to be published with those flaws disclosed.

We reviewers feel obligated to participate in peer review because we been conditioned to believe in its importance for protecting scientific integrity, but doing so requires us to work alongside reviewers who abuse the subjectivity and anonymity of peer review. We witness the trauma of those who receive toxic feedback while our obligation to protect the identities of those writing toxic feedback (a condition of participating in the system) makes us involuntarily complicit.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to have survived peer review co-opted for stratification justify perpetuating this system like homeowners protecting their exclusive neighborhoods and cities. We talk about cherishing the traditions and character of our communities. We attribute problems to the inevitable consequences of overcrowding and forces beyond our control. We respond to criticisms of the system as attacks on ourselves, our friends, and others in our community who have served as reviewers, program chairs, and editors. We ask why we should have suffer changes to the system as opposed to asking how we can justify perpetuating a system that makes others suffer.

Many of us blame peer review's toxicity exclusively on the most egregious reviewers. This convenient bogeyman somehow produces enough un-constructive ill-informed feedback to reject most of our papers while at the same time being a small minority that none of our friends or respected colleagues are guilty of being associated with.

Many of us also convince ourselves that others want and need our expertise to "curate" research to separate the *important* from the merely *accurate*. Yet, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, who are also the most vulnerable. Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does.
<!-- Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times. -->

Lastly, many of us excuse our complicity by asserting that there is simply no escape from co-opting peer review for stratification, as  hiring and tenure committees rely on it to rank people.


<!-- But we can -->
---

But, we *can* and *should* conduct peer review for the purpose we say its for: helping authors increase the the objectivity, accuracy, understandability, and completeness of their work; and helping to prevent their audience from being mislead.

The goals of science-first peer review don't require us to ever *reject* research. Rather, peer reviewed should mean that audience has access to reviews by reasonably-objective experts in the field.

When authors submit for science-first peer review, their work should be routed to reviewers based on expertise and availability, so that authors can get feedback as quickly as possible.  Since there's no decision for a committee to make, authors can get access to feedback the moment the first reviewer is ready to share it.

When authors revise their work based on reviewers' feedback, they can re-submit it the revisions and ask reviewers to revise their reviews. At any point, authors can publish their work and link it to the reviews of that revision. If some reviewers had not updated the reviews for the latest revision, the authors could link the outdated review with the outdated revision and a diff showing what changed since the review was written. Reviewers might write their reviews anonymously, as most do today, but since science-first peer review is less adversarial and the review itself would be published, many might share their identities.

By giving readers access to reviewers' insights into the discoveries and limitations of research, and the source of these insights, we would be giving the press and the public are actually looking for when they ask whether research has been peer reviewed. We would also be helping them to understand that saying work is "peer reviewed" is of little value if the entirety of reviewers' opinions are compressed into a binary "accept" or "reject" decision.


<!-- We should be protectors of truth, not gatekeepers of speech. -->

In addition to reviewing work when it is ready to publish, science-first peer review is well suited to evaluations that begin when the work is at the experimental design phase, *prior* to the execution of the experiment. Currently, the only peer review typically performed prior to an experiment's execution is ethical review, and typically only for human subjects experiments. Prior review would ensure that reviewers' initial feedback is unbiased by an experiments' outcomes, and would give researchers the opportunity to fix methodological flaws before they start conducting their experiments. Our current peer review regime is regrettably wasteful in identifying flaws in experiments only after the resources to conduct the experiment have been exhausted, and often after some of the experts who conducted the research have graduated or moved onto new jobs.

Science-first peer review could also allow us to rethink how we distribute the limited presentation space and time slots conferences. Reviewers currently try to curate a set of papers they think attendees will want to see, or that they think we *should* want to see. Conferences could instead ask prospective attendees directly which of the candidate presentations we want to see and curate a schedule optimized to give us what we *actually* want. Conferences could host presentations with the most interest in the largest lecture halls; they could give presentations with a small but devoted audience more intimate spaces; they could offer online or recorded presentations of papers with insufficient interest to justify physical space. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

---

There will be resistance to science-first peer review. We are accustomed to conducting peer review *for* conferences and journals, when the two stakeholders we really should be working for are authors and their audience. Our collective complicity in using peer review for stratification has left us with few objective peers to question this choice.

Fortunately, there is little in today's peer review system that prevents those who want to request and participate in science-first peer review from doing so. We should establish science-first review committees, volunteer to review for them, and submit to them.

And, so long as we participate in peer review that puts stratification over science, we can move forward simply by being honest with the press and the public about what peer review really means when we make that choice. When we can not do as we say, we must at least say what we do, so that objective outsiders can render their opinions.

<!-- Similarly, the more forthcoming we can be with aspiring scientists about the toxicity of social stratification in science, and the role of peer review in that system, the more might survive to help us change it. -->

<!-- And, for better or worse, creating science-first peer review will not bring an end to stratification. The systems and organizations that feed on stratification will continue to find ways to sort us into "[grades](https://awards.acm.org/advanced-member-grades)". There will always be temptations for scientists to put themselves above science. 
 -->
<!-- (While few of us think of ourselves as proponents of social stratification, we become implicit advocates when conforming to the pressure to publicly congratulate peers elevated to the next strata.) We should at least acknowledge and reckon with social stratification in science because our failure to do so has made it harder for aspiring scientists to survive it. -->
 


<!-- em — , en –   …  -->
