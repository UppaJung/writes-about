## Rejecting Ranking-Focused Peer Review

We scientists often depict the goal of peer review as protecting scientific integrity: ensuring research is presented clearly, completely, objectively, and accurately. The press and public use peer review to determine whether to trust research – as do scientists who lack domain expertise – and so preventing research from misleading *should* be paramount.

But in much of science integrity is not the predominant goal of peer review—in practice peer review is used to rank the importance research and, by extension, each other.<!-- *ranking*. Peer review is the linchpin of the system that we scientists use
--> The foundation of this stratification system are journals and conferences that publish research, each with its own brand. They signal their brand's strata of prestige by being selective in which submissions they accept. We rank scientists by their ability to accumulate publications with exclusive brands, deciding who is worthy of acclaim, jobs, attention, and positions on peer review committees.

To be exclusive, peer review committees must reject many submissions that merely satisfy scientific-integrity requirements, so they add requirements that research to be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

This co-option of peer review for ranking can conflict with, and subvert, scientific integrity. And so, ironically, when we depict the purpose of peer review as preventing scientists from unwittingly misleading ourselves and the public, and elide how we use peer review for ranking, we mislead ourselves and the public.

<!-- Ranking conflicts with integrity -->
---

One way co-opting peer review for ranking misleads us is by *biasing what gets published*. Submissions that confirm the hopes and beliefs of peer reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among other things, this increases the likelihood of false discovery, especially when when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to find specious  support the attractive-but-false hypothesis that the intervention is effective—and get published.

*Ranking undermines how we document our research.* When writing for  peer reviews who demand that papers are interesting and that writing flows well, authors will be tempted to elide mundane details of experimental designs that might bore reviewers, even if those details would be needed to replicate the experiment. They will instead dedicate that space to convince reviewers the work has a significant contribution. Authors will also be tempted to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. Authors may also be tempted to deceive others, and even themselves, to believe that they had planned to conduct hypothesis tests prior to observing data when in fact their decision to conduct a test was influenced by differences they observed in the data.

*Ranking makes papers less readable*, as reviewers expect papers to look like previously-accepted papers even though great scientific writing can take many forms. For example, reviewers are likely to reject papers with fewer-than-average citations even if the topic has few works to cite. This can compel authors to add dozens of unnecessary citations to avoid giving reviewers the impression they are unfamiliar with related work in the field. It's no wonder papers have evolved to contain ever longer chains of citation numbers, most of which authors add solely to reduce the chance of rejection. If authors had the freedom to write papers to inform, rather than conform, we could use that space to provide more insight into the smaller set of truly relevant prior work.

*Ranking can make writing more subjective*, such as when papers are expected to include discussions that go beyond factual reporting of results to speculate about broader implications and impact of those results. Authors who are reluctant to add such subjectivity to scientific writing are compelled to conform to this tradition even if they find it unscientific.

*Ranking delays the spread of knowledge* because ranking-focused peer review is often structured as an admissions process. Admissions processes force authors to submit research at specific deadlines and must give reviewers time to review enough submissions to rank them relative to each other, then to compare papers and discuss acceptance cut-offs. Each submission takes months, and papers must be re-submitted until it reaches a publication that considers the research worthy of its brand. A paper that needs to be submitted three times may take a year to be accepted and longer to publish.

*Ranking disrupts science by adding delays and the need for resubmissions.* After months of waiting for reviews, authors are are expected to come back to speed on what we have written, revise per the subjective concerns of reviewers, and (unless lucky enough to be accepted in the first round) re-submit to a different set of reviewers with different (often conflicting) expectations, paper formatting rules, and other customs.

*Ranking creates incentives to lower our scientific integrity standards*, because lower strata conferences and journals may find it financially necessary publish more papers than they have submissions that meet scientific-integrity standards. At the lowest strata, papers may be accepted without any actual review, so long as authors pay publication fees or conference-registration fees.

---

Co-opting peer review for ranking also biases science by biasing who survives the training process to become a scientist, excluding those not "thick-skinned" or perseverant enough to survive mostly negative feedback from the community they are trying to join.

Peer review co-opted for ranking cannot help but have excessively negative feedback, as most papers will be rejected, and most reviews will recommend rejection. Reviewers are obligated provide feedback to justify their recommendations to themselves, to their fellow committee members, and to the authors. Committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but the overriding feedback must still justify rejection. Rejecting a majority of papers will necessarily provide more negative feedback to the community than positive feedback, which is harmful to the community's mental health and especially to those most junior and least powerful.

The systems we've evolved to support ranking-focused peer review are also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific-integrity requirements when previously rejected. We write feedback to authors who may be content to send their work unchanged to the next set of reviewers.

We feel obligated to participate in peer review because we believe in its importance for protecting scientific integrity. We work alongside reviewers who abuse the subjectivity and anonymity of peer review. We are obligated to protect their identities, but powerless to prevent the trauma their negative feedback inflicts on others. When peer review is primarily used to rank is impossible to contribute without becoming complicit.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to have survived ranking-focused peer review justify perpetuating this system like homeowners protecting their exclusive neighborhoods and cities. We talk about protecting the traditions and character of our communities. We attribute problems to the inevitable consequences of overcrowding and forces beyond our control. We respond to criticisms of the system as attacks on ourselves, our friends, and and others in our community who have served as reviewers, program chairs, and editors. We ask why we should have suffer changes to the system as opposed to asking how we can justify perpetuating a system that makes others suffer.

Many of us blame peer review's toxicity exclusively on the most egregious reviewers. This convenient bogeyman somehow produces enough un-constructive ill-informed feedback to reject most of our papers while at the same time being a small minority that none of our friends or respected colleagues are guilty of being associated with.

Many of us also deceive ourselves that others want and need our expertise to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable. Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does.
<!-- Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times. -->

Lastly, many of us excuse our complicity by asserting that there is simply no escape from using peer review for ranking and stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

<!-- But we can -->
---

But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

We can start by creating new science-first peer review regimes that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness as a service to authors. When authors and reviewers cannot agree on how to best improve a submitted work, or cannot agree on when or whether the work is ready to share, we can publish reviewers' dissenting opinions in concert with, or even attached to, the published work. We can publish supporting opinions, too. Such insight into the credibility of research is what the press and the public are actually looking for when they ask whether research has been peer reviewed. Better that we live up to their expectations than lower their expectations.

Among other benefits, science-first peer review would allow those of us unwilling to be complicit in stratification to serve science by participating in peer review.

Science-first peer review could include the evaluation of experimental designs *prior* to their execution. Currently, the only peer review performed prior to execution is ethical review for human subjects experiments. Prior review would ensure reviews are unbiased by an experiments' outcomes. It would give researchers the opportunity to fix methodological flaws identified by review prior to conducting their experiments. Our current peer review regime is regrettably wasteful in identifying flaws in experiments only after the resources to conduct the experiment have been exhausted, and often after some of the experts who conducted the research have graduated or moved onto new jobs.

Science-first peer review could also allow us to rethink how we distribute the limited presentation space and time slots conferences. Reviewers currently rank papers to curate which they think attendees will want to see, or which they think we *should* want to see. Conferences could instead ask prospective attendees directly which of the candidate presentations we want to see and create a schedule optimized to give us what we *actually* want. Conferences could host presentations with the most interest in the largest lecture halls; they could give presentations with a small but devoted audience more intimate spaces; they could offer online or recorded presentations of papers with insufficient interest to justify physical space. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

---

Along with the verbal irony that we mislead others when we depict the purpose peer review as preventing science from misleading, there is an irony to our situation as well. Our collective complicity in co-opting peer review for ranking has left us no objective peers to suggest we reject this choice.

The more honest we can be in depicting peer review to outsiders, the more they might help inspire us to change. The more forthcoming we can be with aspiring scientists about the toxicity of social stratification in science, the more might survive to help us change it.

And, for better or worse, creating science-first peer review will not bring an end to stratification. The systems and organizations that feed on stratification will continue to find ways to sort us into "[grades](https://awards.acm.org/advanced-member-grades)". There will always be temptations for scientists to put themselves above science. But, ensuring our scientific processes serve science first, and our not flawed against scientific goals by design is imperative.

<!-- (While few of us think of ourselves as proponents of social stratification, we become implicit advocates when conforming to the pressure to publicly congratulate peers elevated to the next strata.) We should at least acknowledge and reckon with social stratification in science because our failure to do so has made it harder for aspiring scientists to survive it. -->
 


<!-- em — , en –   …  -->
