## Let's Peer Review as we Say, Not as we Do

We scientists say we practice peer review to protect the scientific integrity of research: employing objective experts to identify flaws to fix or other improvements to suggest, and reducing the risk that the research will be confuse or mislead its audience. Having taught the press and public that peer review is a signal that research is credible, preventing research from misleading *should* be our paramount goal in conducting it.

But in much of science, what we are actually doing when we peer review research is to rank it relative to other research competing for attention and, by extension, to rank our fellow scientists relative to each other. Peer review committees are run by journals and conferences, each of which has its own brand, and most of which signal their strata of brand prestige by being selective in accepting submissions. They are part of a stratification system in which we scientists compete accumulating publications with these exclusive brands. <!-- Our ability to accumulate publications with exclusive brands determines our career trajectory and our share of acclaim, attention, and – yes – positions on peer review committees. --> To be exclusive, peer review committees with too many submissions of sufficient integrity must determining which will be the most *interesting* to their audience, rejecting those that lack sufficient "novelty", "impact", or that particularly nebulous catch-all (really a *drop-all*) of a "significant contribution".

There are myriad ways in which our using peer review to stratify research undermines scientific writing, and is harmful to scientists and to science itself. So when claim peer review protects scientific integrity, preventing scientists from misleading ourselves and the public, we are mischaracterizing our current practice of peer review in a way that misleads ourselves and the public.

Ranking and curating research is a vestigial function of peer review. It served a purpose in the era when exchanging information was expensive: when research was printed on paper, traveled by mail, and was stored in libraries; when asking distant peers what to read next incurred the cost of a long-distance phone call. In a era with free publication servers we don't need peer reviewers to determine what is worth publishing. In an era with search engines, social sharing, micro-blogging, and myriad other mechanisms to filter through the information that competes for our attention, we don't need peer reviewers to be arbiters of fashion who tell us what is and isn't worth reading.

In this manifesto, I'll review some of the harms caused by using peer review to stratify research (parts 1 & 2) and explain how we can excise this vestigial function so that peer review truly serve science.

#### 1. Ranking research subverts scientific integrity

There are many ways using peer review to rank research, and determine which research is interesting enough to publish, harms scientific writing and causes science to mislead ourselves and others.

**Rejecting uninteresting research biases published results**. Submissions that confirm the hopes and beliefs of reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among the many consequences is the potential for false discovery, especially when when peer review committees deem null results insufficiently interesting to publish. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to find specious support the attractive-but-false hypothesis that the intervention is effective—and get published.

**Favoring interesting research causes authors to write less scientifically**. It inspires authors to elide mundane details of experimental designs that might bore reviewers, even if those details would be needed to replicate their experiments. It provokes them to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. It tempts authors to deceive others, and even themselves, into believing that hypothesis tests they conducted on data they found interesting were planned before they had seen the data. It motivates authors to aggrandize their research to look more important. Many publications even explicitly ask that authors go beyond factual reporting of results to speculate about their research's impact and importance.

**Making reviewers gatekeepers changes the audience research is written for**. When authors must craft papers to be accepted by reviewers they must make sacrifices in serving their intended audience. For example, as authors we often add volumes of citations of no value to anyone but our reviewers. We do so to signal to reviewers that we are experts in the field and to reduce the chance of being faulted for failing to cite a reviewer's favorite work. Since reviewers expect papers to have at least as many citations as previously-accepted work, we add even more unnecessary citations when writing on a relatively new topic for which there is little related work. As authors write to conform, not inform, papers have grown to meet reviewers' ever increasing expectations for citation counts, and with cited work receiving less explanation of why its relevant to help the non-expert reader. We also conform by maximizing the amount of information that can fit of a page limit (to maximize the "contribution" needed for acceptance), and using our full page limit, rather than publishing shorter (or longer) papers.

**Competitively ranking research delays the spread of knowledge** because comparing submissions takes time. Authors must often wait to submit their research until a common deadline. Reviewers need time to read a cohort of submissions to compare them against each other. Committees need time to collectively discuss which papers to accept. These delays can add months to each submission cycle. Research that needs to be submitted three times to find a publication that considers it on brand may take over a year to be accepted and longer to publish.

**Reviewing delays disrupt authors' work and degrade our memories**. Time decays our ability to recall how we conducted our experiments, our observations during the experiments, and the tooling we used to process data and document our results. By the time have access to reviewers' feedback identifying what we failed to document, we may have already forgotten details that reviewers noticed we had elided. Coming back up to speed on that which we do remember also takes time and effort. Unless we are lucky enough to have our work accepted on its first submission, we must disrupt our other work to revise our submission to target a different publication, with different reviewers, with different interests, paper formatting rules, and other expectations.

**Standards for ranked peer review vary and are opaque to outsiders**. Every journal and conference has different standards and those standards change over time. Even the highest-strata conferences in my field are known to override some reviewer's integrity concerns if others believe the work will draw interest and attention. Lower-strata conferences are known to publish work they know to be flawed to meet financial or attendance goals. At the bottom strata, submissions may only appear to be peer reviewed, receive no feedback from reviewers, and may be published regardless of content so long as authors pay publication fees.

<!-- --- -->
<!-- **Co-opting peer review to rank research has broader harms.** -->
#### 2. Ranking research is harmful to scientists and science itself

Co-opting peer review to serve a stratification system has broader harms.

Conferences and journals that reject a majority of papers will necessarily provide more negative feedback to the community than positive feedback. Peer review committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but the overriding feedback must still justify rejection if a paper is to be rejected. 

The predominance of negative feedback biases who becomes a scientist. The scientific community loses talented aspiring scientists who are uncomfortable promoting their work as important, or who are too "thin-skinned" or "insufficiently perseverant" to discard feedback that conveys their work is uninteresting and unimportant. Those more comfortable promoting the importance of their work or whose sheer self confidence allows them to dismiss negative feedback survive.

Having countless peer review committees of different strata is also burdens us with unnecessary work as reviewers. We are expected to review papers that had already met scientific-integrity requirements when previously rejected. We are asked to write feedback to authors who may be content to send their work unchanged to the next set of reviewers with different subjective expectations of what is important.

Being part of a system that produces toxic feedback for authors is also toxic to us as reviewers. We must work alongside reviewers who abuse the subjectivity and anonymity of peer review. We witness the trauma of those who receive toxic feedback while our obligation to protect the identities of those writing toxic feedback (a condition of participating in the system) and can't help but feel involuntarily complicit in harm, even if we volunteered with the intent of serving science.
<!-- 
If only we could perform our scientific duties without participating in a stratification system toxic to science! -->

Alas, many of us have a hard time even imagining what peer review would be like if our job were not to be arbiters of what should be published.

<!-- --- -->

#### 3. Conducting *scientific* peer review


When the most diligent members of the science press write about new scientific findings, they conduct their own investigation into their credibility: reaching out to objective scientific experts, asking if they agree with the findings, and quoting them directly so that readers can draw their own conclusions. We scientists should recognize that this approach does more to protect scientific integrity than the gatekeeping rituals we call peer review. We should emulate this model, especially as many reporters who cover scientific research are not afforded the time to be so diligent.

We should adopt a practice of scientific peer review in which reviewers aren't arbiters of fashion, gatekeeping whether research is worthy of being published, but where their insights into the integrity of the research become public when the research is published. In other words, calling work *scientifically* peer reviewed should mean that the audience has access to the insights of objective scientific peers.

Just as press sources may choose to be anonymous, pseudonymous ("deep throat"), or named, reviewers could also choose how to be identified in their reviews. Reviewers who choose anonymity to avoid retribution for rejecting authors' research may no longer need to do so when they are not acting as gatekeepers of what gets published.

Before publishing, authors can use reviewers' feedback to improve their work: revising statements that might mislead, disclosing limitations their audience might otherwise be unaware of, or making other changes to ensure the work is accurate, complete, and understandable. Reviewers should also be able to update their reviewers. When authors' address their concerns, they should be able to note this, or even remove concerns that no longer apply. If authors make changes that introduce new concerns, they should be able to note that as well.

Should authors want to publish a revision that is newer than the last version a reviewer addressed, they must make that prior version available as well. This allows their audience to see what content has changed since that reviewer reviewed the work.

Authors will not always agree with reviewers' concerns. Making reviews public allows the audience to note this disagreement, just as they would if a reporter quoted a dissenting expert. Authors can rebut that feedback, as could other reviewers.

Scientific peer review could even be applied prior to research being conducted, when authors have settled on an experimental approach. Reviewing experimental designs before execution would give researchers the opportunity to fix methodological flaws before consuming the resources required to conduct their experiments. Our current approach identifies experimental flaws only after the work is complete and many of the experts who conducted the research have graduated or moved onto new jobs. Reviewing prior the the experiment also has the benefit of ensuring that reviewers' initial feedback is unbiased by the results.

To make *scientific* peer review a reality within a field or subfield, we need to create peer review committees dedicated to serving science, that will pair authors work with objective peers. They would act less like committees in that they would not vote to decide which papers are accepted or rejected, but they would likely need to agree on practices such as workload, timelines for initial feedback, and the number and scope of revisions that authors can reasonably expect reviewers to respond to.


<!-- 1. to help the *authors* improve the research, making it more objective, accurate, and understandable, and 
2. to ensure the research does not mislead its audience, identifying flaws that others might have missed and that others might overlook and building confidence in research the reviewer believes to be well conducted. -->

<!-- *Scientific* peer review requires reviewers to fulfill these two duties to authors and their audience. -->

 <!-- Scientific peer review does what we say peer review should: giving the press, public, and any other audience for the research objective insight into its scientific integrity of research to ensure it does not mislead us. It is a more transparent process than one in which the audience only learns that the work was accepted or rejected. -->


<!-- The act of *scientific* peer review requires that we connect authors' research to fellow scientists who are sufficient knowledgeable, objective, and otherwise capable of providing (ideally timely) feedback, and that reviewers fulfill their duty to both authors of the research and its audience. Hopefully, authors will use reviewer's feedback to revise their work, fixing flaws and making other improvements, and reviewers will update their feedback to account for these changes.

To publish their work as scientifically peer reviewed, authors must only ensure their audience has access to the reviews. Should authors want to publish a revision that is newer than the last version one or more reviewers have provided feedback on, they must make the reviewed versions available so that their audience can see what content was reviewed and what content has changed since each reviewer last provided feedback, and hence has not been evaluated by some reviewers. Scientific peer review does what we say peer review should: giving the press, public, and any other audience for the research objective insight into its scientific integrity of research to ensure it does not mislead us. It is a more transparent process than one in which the audience only learns that the work was accepted or rejected.

When peer review is not a means for research to compete for attention, and reviewers are not gatekeepers who dole out prestige, peer reviewers and authors have less cause to see each other as adversaries. Reviewers could still write prejudicial feedback, which scientific peer review would require authors would be required to share with their audience, but the authors and other reviewers can rebut that feedback. Reviewers who view their role as cooperative have less cause to remain anonymous than those in adversarial reviewing. Some might prefer to the protection of anonymity when providing feedback that informs the audience of shortcomings in the authors' work, but others may in their own names or using a pseudonym. Those reviewers who identify themselves would provide the audience even more insight into the objectivity and expertise they bring to their reviews. -->




 <!-- Modified content must not be falsely presented as having been peer reviewed. -->

<!-- When authors revise their work they can request that reviewers revise their reviews. Reviewers should be able to submit new revised feedback at any time, such as after reading others' reviews or reading rebuttals or other comments from authors. -->

<!-- FIXME --  Sum up that we should do this -->

<!-- FIXME -- here, talk about benefits beyond toxicity  -->



<!-- FIXME -- good time to point out other uses, such as pre-experiment -->

#### Excising the vestiges of ranking

Practicing peer review for the purpose of ranking is like an anachronistic religious bloodletting ritual, and our journals and conferences are the churches and temples that conduct and perpetuate this rite without scrutiny as to its modern relevance. If authors have access to scientific peer review, there is no reason to conduct peer review for either of these institutions.

Without the pretense of protecting scientific integrity, journals are effectively awards committees that endow prestige onto work within their field or subfield. Those who like awards committees and feel they serve the community can contributing to them. The rest of us should feel no obligation.

Conferences do serve a purpose: bringing researchers together and providing a forum for researchers to present their work. We should be suspicious of the use of peer review committees to curate which research gets presented at conferences, as most review committees are biased in favor of more senior researchers over those with newer ideas and more vulnerable positions. Better to ask prospective attendees directly which of the candidate presentations we are interested in attending and curating a schedule optimized to give us what we want, not want reviewers think we want: allocating the largest spaces to the presentations with the largest audiences and the more intimate spaces to those with small but passionate audiences. Authors of work that have insufficient interest to warrant presentation space can still use the conference to meet people who want to talk to them about their work.

These institutions have the most to lose if we start to conduct *scientific* peer review, putting the needs of those of us who write, consume, and review scientific research above their need to maintain a valuable brand. 

Alas, most of us feel deep loyalty to these institutions and their brands and the validation they give us. We see criticisms of these institutions as attacks on ourselves, our friends, and others in our community who have served as reviewers, program chairs, and editors. Our collective loyalty and complicity with these institutions makes it hard fo us to deprogram ourselves and reckon with the harms they cause.

Some of us avoid reckoning with our complicity by placing all the blame for peer review's toxicity on the most egregious reviewers. This convenient bogeyman somehow produces enough un-constructive ill-informed feedback to reject most of our papers while at the same time being a small minority that none of our friends or respected colleagues are guilty of being associated with.

Others justify perpetuating these institutions in much the same way that homeowners justify protecting their exclusive neighborhoods. We talk about cherishing the traditions and character of our communities. We attribute problems to the inevitable consequences of overcrowding and forces beyond our control.




---




<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->



<!-- Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times. -->

Lastly, many of us excuse our complicity by asserting that there is simply no escape from the system that demands stratification. Journals and conferences need exclusive brands to attract submissions. Hiring and tenure committees need exclusive publications to rank candidates. We simply can't imagine peer review any other way.

<!-- But we can -->
---

---

There will be resistance to science-first peer review. We are accustomed to conducting peer review *for* conferences and journals, when the two stakeholders we really should be working for are authors and their audience. Our collective complicity in using peer review for stratification has left us with few objective peers to question this choice.

Fortunately, there is little in today's peer review system that prevents those who want to request and participate in science-first peer review from doing so. We should establish science-first review committees, volunteer to review for them, and submit to them.

<!-- All we need to start conducting science-first peer reviews is a quorum of scientists ready to put authors, their audience, and reviewers before the journals, conferences, and the professional societies that have historically controlled peer review committees and made peer review serve their purposes. We simply need enough of us willing to conduct reviews, and a mechanism for pairing research to reviewers who are sufficiently objective, sufficiently knowledgeable, and who are able to provide timely feedback.
 -->

And, so long as we participate in peer review that puts stratification over science, we can move forward simply by being honest with the press and the public about what peer review really means when we make that choice. When we can not do as we say, we must at least say what we do, so that objective outsiders can render their opinions. That is, after all, what science should be about.

<!-- Similarly, the more forthcoming we can be with aspiring scientists about the toxicity of social stratification in science, and the role of peer review in that system, the more might survive to help us change it. -->

<!-- And, for better or worse, creating science-first peer review will not bring an end to stratification. The systems and organizations that feed on stratification will continue to find ways to sort us into "[grades](https://awards.acm.org/advanced-member-grades)". There will always be temptations for scientists to put themselves above science. 
 -->
<!-- (While few of us think of ourselves as proponents of social stratification, we become implicit advocates when conforming to the pressure to publicly congratulate peers elevated to the next strata.) We should at least acknowledge and reckon with social stratification in science because our failure to do so has made it harder for aspiring scientists to survive it. -->
 


<!-- em — , en –   …  -->
