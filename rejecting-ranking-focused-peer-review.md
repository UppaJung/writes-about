## Rejecting Ranking-Focused Peer Review

We scientists often depict the goal of peer review to be protecting scientific integrity: ensuring research is presented clearly, completely, objectively, and accurately. The press and public for use peer review to determine whether to trust research – as do scientists who lack domain expertise – and so preventing research from misleading should, indeed, be paramount.

Alas, in much of science, integrity is not the predominant goal of peer review—in practice the overriding goal is *ranking*. Peer review is the linchpin of the system that we scientists use to rank research and, by extension, to rank each other.

Our peer review committees are run by journals and conferences, each with its own brand. They signal their brand's strata of prestige by retaining distinguished reviewers and ensuring acceptance rates are low enough to convey exclusivity. Peer review for these journals and conferences is the foundation of science's stratification system: we rank the importance of research by where it is published; we rank the importance of scientists by their ability to accumulate publications with prestigious brands; then we use these rankings to decide who is worthy of acclaim, jobs, attention… and positions on peer review committees.

Maintaining exclusivity requires reviewers to reject papers that merely satisfy scientific-integrity requirements. Review committees create exclusivity by requiring research to be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

Our co-option of peer review for ranking conflicts with, and subverts, scientific integrity. And so ironically, when we depict peer review's purpose as preventing scientists from unwittingly misleading ourselves and the public, and elide the its use for ranking, we are guilty of misleading ourselves and the public.

<!-- Ranking conflicts with integrity -->
---

One way ranking subverts scientific integrity is by *biasing what gets published.* Submissions that confirm the hopes and beliefs of peer reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among other things, this increases the likelihood of false discovery, especially when when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to find specious  support the attractive-but-false hypothesis that the intervention is effective—and get published.

*Ranking undermines scientific integrity when we document our research.* When writing for ranked peer review, authors will be tempted to elide mundane details of experimental designs that reviewers might find uninteresting, even if those details would be needed to replicate the experiment, in favor of writing that will convince reviewers the work has a significant contribution. Authors will also be tempted to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. They may deceive others, and even themselves, that they had planned to conduct hypothesis tests prior to observing data when in fact their decision to conduct a test was influenced by differences they observed in the data.

*The expectations of ranking-focused reviewers impose unneeded homogeneity*, as reviewers expect papers to look like previously-accepted papers even though great scientific writing can take many forms. For example, reviewers are likely to reject papers with fewer-than-average citations even if the topic has few works to cite. This can compel authors to add dozens of unnecessary citations to avoid giving reviewers the impression they are unfamiliar with related work in the field. It's no wonder papers have evolved to contain ever longer chains of citation numbers, most of which authors add solely to reduce the chance of rejection. If authors had the freedom to write papers to inform, rather than conform, we could use that space to provide more insight into the smaller set of truly relevant prior work.

Another such expectation is that papers to include discussions that go beyond factual reporting of results to speculate about broader implications and impact of those results. Authors who are reluctant to add such subjectivity to scientific writing are compelled to conform to this tradition even if they find it unscientific.

*Ranking delays the spread of knowledge* because ranking-focused peer review is often structured as an admissions process. Admissions processes force authors to submit research at specific deadlines and must give reviewers time to review enough submissions to rank them relative to each other, then to compare papers and discuss acceptance cut-offs. Each submission takes months, and papers must be re-submitted until it reaches a publication that considers the research worthy of its brand. A paper that needs to be submitted three times may take a year to be accepted and longer to publish.

*Delays and re-submissions are disruptive to conducting science.* After months of waiting for reviews, authors are are expected to come back to speed on what we have written, revise per the subjective concerns of reviewers, and (unless lucky enough to be accepted in the first round) re-submit to a different set of reviewers with different (often conflicting) expectations, paper formatting rules, and other customs.

At the lower strata are publications that peer review in name only, accepting any paper for which authors will pay publication fees or conference-registration fees. When we use *peer reviewed* synonymously with *accepted for publication* the term loses all value as an indicator of scientific integrity.

---

Peer review co-opted for ranking cannot help but have excessively negative feedback, as most papers will be rejected, and most reviews will recommend rejection. Reviewers are obligated provide feedback to justify their recommendations to themselves, to their fellow committee members, and to the authors. Committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but the overriding feedback must still justify rejection. Rejecting a majority of papers will necessarily provide more negative feedback to the community than positive feedback, which is harmful to the community's mental health and especially to those most junior and least powerful.

The systems we've evolved to support ranking-focused peer review are also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific-integrity requirements when previously rejected. We write feedback to authors who may be content to send their work unchanged to the next set of reviewers.

We feel obligated to participate in peer review because we believe in its importance for protecting scientific integrity. We work alongside reviewers who abuse the subjectivity and anonymity of peer review. We are obligated to protect their identities, but powerless to prevent the trauma their negative feedback inflicts on others. When peer review is primarily used to rank is impossible to contribute without becoming complicit.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to have survived ranking-focused peer review justify perpetuating this system like homeowners protecting their exclusive neighborhoods and cities. We talk about protecting the traditions and character of our communities. We attribute problems to the inevitable consequences of overcrowding and forces beyond our control. We respond to criticisms of the system as attacks on ourselves, our friends, and and others in our community who have served as reviewers, program chairs, and editors. We ask why we should have suffer change as opposed to asking how we can justify perpetuating a system that makes others suffer.

We blame peer review's toxicity exclusively on the most egregious reviewers. This convenient bogeyman somehow produces enough un-constructive ill-informed feedback to reject most of our papers while at the same time being a small minority that none of our friends or respected colleagues are guilty of being associated with.

We also deceive ourselves that others want and need our expertise to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable. Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does.
<!-- Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times. -->

Lastly, we may excuse our complicity by believing that there is simply no escape from using peer review for ranking and stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

<!-- But we can -->
---

But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

We can start by creating new peer review regimes that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness as a service to authors. When authors and reviewers cannot agree on how to best improve a submitted work, or cannot agree on when or whether the work is ready to share, we can publish reviewers' dissenting opinions in concert with, or even attached to, the published work. We can publish supporting opinions, too. Such insight into the credibility of research is what the press and the public are actually looking for when they ask whether research has been peer reviewed. We should live up to this ideal.

Redesigning peer review without ranking also presents other opportunities to serve science. For human-subjects research, reviewing experimental designs prior to their execution could ensure reviewers are unbiased by the experiments' outcomes and give researchers the opportunity to fix methodological flaws identified by review before conducting their experiments. It's regrettably wasteful that our current peer review regime identifies flaws in experiments only after the resources to conduct the experiment have been exhausted, and often after some of the experts who conducted the research have graduated or moved onto new jobs.

Rethinking peer review could also allow us to rethink how we distribute the limited presentation space and time slots conferences. Reviewers currently rank papers to curate which they think attendees will want to see, or which they think we *should* want to see. Conferences could instead ask prospective attendees directly which of the scientifically-reviewed candidate presentations they want to see and create a schedule optimized to give us what we *actually* want. Conferences could host presentations with the most interest in the largest lecture halls; they could give presentations with a small but devoted audience more intimate spaces; they could offer online or recorded presentations of papers with insufficient interest to justify physical space. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

Once it is possible to participate in scientific peer review without ranking, the community of reviewers can include those eager to serve science but unwilling to be complicit in perpetuating stratification. Those who still want to rank papers for curation into prestigious publications, or for awards, can still choose to.

---

Ironically, one reason we have a hard time reckoning with our role in this stratification system we perpetuate is that we lack an outsiders' objective perspective—our peers are also complicit. We have collectively deceived ourselves that this is all okay.

And, for better or worse, removing ranking from scientific peer review, or even from scientific research, will not bring an end to stratification. The systems and organizations that rely on ranking-focused peer review for stratification will adapt and survive, as they have in the past. They are also good at manufacturing demand for stratification. In fact, many of the professional societies that host our conferences and publish our journals already have committees dedicated exclusively to stratifying their membership into "[grades](https://awards.acm.org/advanced-member-grades)". (While few of us think of ourselves as proponents of social stratification, we become implicit advocates when conforming to the pressure to publicly congratulate peers elevated to the next strata.)

We should at least acknowledge and reckon with social stratification in science because our failure to do so has made it harder for aspiring scientists to survive it. Too much talent is lost because we expect researchers to be great scientists, but to also be "thick-skinned" and perseverant enough to survive in a system that has prioritized ranking over science.

At a bare minimum, we must at least be honest with outsiders about how we actually practice peer review in science. We should also not be surprised when those objective outsiders reach a different conclusion when presented with the choice to allow ranking to continue to co-opt scientific integrity in peer review: *strong reject*.

<!-- em — , en – -->
