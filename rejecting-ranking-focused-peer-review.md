## Rejecting Ranking-Focused Peer Review

We scientists like to propagate the convenient myth that the primary purpose of peer review is one of scientific integrity: to ensure research informs without misleading. Indeed, ensuring presented clearly, completely, objectively, and accurately *should* be paramount to scientists.

Alas, the overriding purpose of peer review, as we actually practice it in most of science, is *ranking*.

Peer review is the linchpin of the system that we scientists use to rank the value of research and, by extension, to rank each other. Our peer review committees are run by journals and conferences, each with its own brand and tier of prestige. They chose standards of acceptance and reviewers to signal that prestige.

Peer review for these journals and conferences is the foundation of science's stratification system: we rank the importance of research by where it is published; we rank the importance of scientists by their ability to accumulate publications with prestigious brands; then we use these rankings to decide who is worthy of acclaim, jobs, trust, and attention.

This system requires reviewers to reject papers that merely satisfy scientific-integrity requirements, lest the top strata would be too inclusive. Review committees create exclusivity through requirements such that research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

<!-- Ranking conflicts with integrity -->
---

Our use of peer review for ranking renders scientific integrity secondary and ranking requirements conflict with, and subvert, scientific integrity.

*Ranking requirements bias what gets published.* Submissions that confirm the hopes and beliefs of peer reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Consider, for example, the consequences when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to support the attractive-but-false hypothesis that the intervention is effective—and get published.

*Ranking requirements can make writing less scientific.* When writing for ranked peer review, authors elide mundane details of experimental designs that reviewers might find uninteresting, even if those details would be needed to replicate the experiment. Authors highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting.

*The expectations of rank-focused reviewers impose unneeded homogeneity*, as they are more likely to reject papers that don't look like previously-accepted papers. For example, they are likely to reject papers with fewer-than-average citations even if the topic has few works to cite. This can compels authors to add dozens of unnecessary citations to avoid giving reviewers the impression they are unfamiliar with related work in the field. It's no wonder papers have evolved to contain ever longer chains of citation numbers, most of which are authors add solely to reduce our chance of rejection. If authors had the freedom to write papers to inform, rather than conform, we would use that space to provide more insight into the smaller set of truly relevant prior work.

Similarly, reviewers expect papers to include discussions that speculate about implications. Those who are reluctant to add such  subjectivity to scientific writing are compelled by to do so by tradition.

*Ranking delays the reporting of findings* because rank-based peer review is often structured as an admissions process. Admissions processes for authors force authors to submit at specific deadlines and must give reviewers time to review enough submissions to rank them and discuss acceptance cut-offs. Each submission takes months, and papers must be re-submitted until it reaches a publication that considers the research worthy of its brand. A paper that needs to be submitted three times may take a year to be accepted and longer to publish.

*Delays and re-submissions are disruptive to conducting science.* After months of waiting for reviews, authors are are expected to come back to speed on what we'd written, revise per the subjective concerns of reviewers, and (unless we were lucky enough to be accepted in the first round), re-submit to a different set of reviewers with different (often conflicting) expectations, paper formatting requirements, and other rules and expectations.

---

Peer review cannot help but be systemically toxic when co-opted for ranking. Reviewers feel obligated to provide feedback that justifies the outcome to their fellow committee members, to themselves, and to the authors—feedback focused on shortcomings when justifying rejection. A system that must reject the majority of papers will provide much more negative feedback to the community than positive feedback, which is harmful to the community's mental health and particularly harmful to the most junior and least powerful members.

The systems we've evolved to support rank-focused peer review are also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific integrity requirements when previously rejected. We write feedback to authors who may be content to send their work unchanged to the next set of reviewers.

We feel obligated to participate in peer review because of its importance for protecting scientific integrity, but it is nearly impossible to do so without becoming complicit in the toxicity of the social stratification system built upon it. We are obligated to work with reviewers who abuse the subjectivity and anonymity of peer review, and obligated to protect their identities, while witnessing the trauma they inflict cause promising aspiring scientists to abandon careers in research.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to survive ranking-focused peer review explain our reasons for perpetuating this system using justifications eerily similar to the ones homeowners use to justify keeping their neighborhoods exclusive. We talk about overcrowding and the loss we would feel if the traditions and character of our communities changed. We ignore the question of whether we are perpetuating a harmful system, and instead ask why we should have to give up that which we are accustomed to. We respond to criticisms of the system as attacks on ourselves, our friends, and our community (fellow reviewers, program chairs, and editors).

Ironically, one reason we have a hard time reckoning with our role in this stratification system we perpetuate is that we lack an outsiders' objective perspective—our peers are also complicit. We have collectively deceived ourselves that this is all okay.

We blame peer review's toxicity exclusively on the most egregious reviewers—a convenient *other* that is somehow among us despite not including any of our friends or respected colleagues. The most toxic reviewers make great scapegoats, but they are merely the most visible symptoms of the very cancer we ourselves are feeding. We collectively receive far more un-constructive ill-informed feedback than this a few egregious reviewers could produce.

We also deceive ourselves by believing that others need our expertise to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable. No self-respecting social scientist would intentionally design a system biased in this way.

Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does. Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times.

Lastly, we may excuse our complicity by believing that there is simply no escape from using peer review for ranking and stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

<!-- But we can -->
---


We can start by creating new peer review processes that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness when research is published. When authors and reviewers cannot agree on how to best improve a submitted work, or cannot agree on when or whether the work is ready to publish, we can publish reviewers' dissenting opinions in concert with, or even attached to, the published work. We can publish supporting opinions, too. Such insight into the credibility of research is what the press and the public are actually looking for when they ask whether research has been peer reviewed.

Redesigning peer review to make science paramount also presents other opportunities. For human-subjects research, reviewing experimental designs prior to their execution could ensure reviewers are unbiased by the experiments' outcomes and give researchers the opportunity to fix methodological flaws identified by review before conducting their experiments.

Rethinking peer review could also allow us to rethink conferences, which currently rank papers to determine which are worthy of presentation slots. We could instead ask prospective attendees which of the scientifically-reviewed candidate presentations they would want to see and create a schedule optimized to give attendees what we want, rather than what reviewers guess we want, or what reviewers think we should want. Presentations with the most interest could be hosted in the largest lecture halls; presentations with a small but devoted audience could appear in more intimate spaces; papers with insufficient interest could be offered online or recorded. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

And once we have removed ranking from scientific peer review, the community of reviewers can grow to include those of us who want to serve science but have been unwilling to do so to avoid complicity in ranking-focused review.

---

For better or worse, removing ranking from scientific peer review, or even from scientific research, will not bring an end to stratification. The systems and organizations that rely on ranking-focused peer review for stratification will adapt and survive, as they have in the past, so long as there is demand. In fact, many of the professional societies that host our conferences and publish our journals already have committees dedicated exclusively to stratifying their membership into "[grades](https://awards.acm.org/advanced-member-grades)" that applies to members who who don't publish any research. (While many of us do not see ourselves as advocates for social stratification, we become implicit advocates when conform to the social pressure to publicly congratulate peers when they reach to the next strata.)

For aspiring scientists, learning to recognize the toxicity of stratification is key to surviving it and, hopefully, surviving to be part of the change.

Until we change how we conduct peer review, each successive generation fo scientists will exclude amazing talent that failed to be "thick-skinned" and perseverant enough to survive in a system that has prioritized ranking over science. Each generation will inflict harm on the next for no better reason that that "we've always done it this way" and we don't know how to change. The only way out is to examine ranking-focused peer review from the perspective of an objective outsider, from which there is only one reasonable conclusion: *reject*.

<!-- em — , en – -->
