## Rejecting Ranking-Focused Peer Review

We scientists like to describe peer review as a process devoted to scientific integrity: ensuring research is presented clearly, completely, objectively, and accurately. Indeed, ensuring research does not mislead should be the paramount goal of peer review; not only is it important within science, but to the press and public peer review is the best available signal of whether research can be trusted.

Alas, the overriding purpose of peer review, as we actually practice it in most of science, isn't integrity—it's *ranking*.

Peer review is the linchpin of the system that we scientists use to rank the value of research and, by extension, to rank each other. Our peer review committees are run by journals and conferences, each with its own brand and tier of prestige. They chose standards of acceptance and reviewers to signal that prestige.

Peer review for these journals and conferences is the foundation of science's stratification system: we rank the importance of research by where it is published; we rank the importance of scientists by their ability to accumulate publications with prestigious brands; then we use these rankings to decide who is worthy of acclaim, jobs, trust, and attention.

This system requires reviewers to reject papers that merely satisfy scientific-integrity requirements, lest the top strata would be too inclusive. Review committees create exclusivity through requirements such that research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

<!-- Ranking conflicts with integrity -->
---

Our use of peer review for ranking renders scientific integrity secondary and ranking requirements conflict with, and subvert, scientific integrity. And so, when we present peer review only as a process to help prevent scientists from unwittingly misleading ourselves and the public (eliding its primary role as a ranking mechanism) we are, ironically, guilty of misleading ourselves and the public.

One reason is that *ranking requirements bias what gets published.* Submissions that confirm the hopes and beliefs of peer reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Consider, for example, the consequences when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to support the attractive-but-false hypothesis that the intervention is effective—and get published.

*Ranking requirements can make writing less scientific.* When writing for ranked peer review, authors will be tempted to elide mundane details of experimental designs that reviewers might find uninteresting, even if those details would be needed to replicate the experiment, in favor of writing that will convince reviewers the work has a significant contributions. Authors will also be tempted to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting.

*The expectations of ranking-focused reviewers impose unneeded homogeneity*, as reviewers are more likely to reject papers that don't look like previously-accepted papers than those that do. For example, reviewers are likely to reject papers with fewer-than-average citations even if the topic has few works to cite. This can compel authors to add dozens of unnecessary citations to avoid giving reviewers the impression they are unfamiliar with related work in the field. It's no wonder papers have evolved to contain ever longer chains of citation numbers, most of which are authors add solely to reduce our chance of rejection. If authors had the freedom to write papers to inform, rather than conform, we would use that space to provide more insight into the smaller set of truly relevant prior work.

Similarly, reviewers expect papers to include discussions that speculate about implications. Those who are reluctant to add such  subjectivity to scientific writing are compelled by to do so by tradition.

*Ranking delays the reporting of findings* because ranking-focused peer review is often structured as an admissions process. Admissions processes for authors force authors to submit at specific deadlines and must give reviewers time to review enough submissions to rank them and discuss acceptance cut-offs. Each submission takes months, and papers must be re-submitted until it reaches a publication that considers the research worthy of its brand. A paper that needs to be submitted three times may take a year to be accepted and longer to publish.

*Delays and re-submissions are disruptive to conducting science.* After months of waiting for reviews, authors are are expected to come back to speed on what we'd written, revise per the subjective concerns of reviewers, and (unless we were lucky enough to be accepted in the first round), re-submit to a different set of reviewers with different (often conflicting) expectations, paper formatting requirements, and other rules and expectations.

---

Peer review cannot help but be systemically toxic when co-opted for ranking. Reviewers feel obligated to provide feedback that justifies the outcome to their fellow committee members, to themselves, and to the authors—feedback focused on shortcomings when justifying rejection. A system that must reject the majority of papers will provide much more negative feedback to the community than positive feedback, which is harmful to the community's mental health and particularly harmful to the most junior and least powerful members.

The systems we've evolved to support ranking-focused peer review are also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific integrity requirements when previously rejected. We write feedback to authors who may be content to send their work unchanged to the next set of reviewers.

We feel obligated to participate in peer review because of its importance for protecting scientific integrity, but it is nearly impossible to do so without becoming complicit in the toxicity of the social stratification system built upon it. We are obligated to work with reviewers who abuse the subjectivity and anonymity of peer review, and obligated to protect their identities, while witnessing the trauma they inflict cause promising aspiring scientists to abandon careers in research.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to have survived ranking-focused peer review justify perpetuating this system like homeowners protecting their exclusive neighborhoods and cities. We talk about overcrowding and protecting the traditions and character of our communities. We ask why we should have to experience change. We respond to criticisms of the system as attacks on ourselves, our friends, and our community (fellow reviewers, program chairs, and editors).

We blame peer review's toxicity exclusively on the most egregious reviewers—a convenient *other* that is somehow among us despite not including any of our friends or respected colleagues. The most toxic reviewers make great scapegoats, but they are merely the most visible symptoms of the very cancer we ourselves are feeding. We collectively receive far more un-constructive ill-informed feedback than this a few egregious reviewers could produce.

We also deceive ourselves by believing that others need our expertise to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable. No self-respecting social scientist would intentionally design a system for identifying that which is new and interesting biased to favor the opinions of those most set in their ways.

Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does. Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times.

Lastly, we may excuse our complicity by believing that there is simply no escape from using peer review for ranking and stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

Ironically, one reason we have a hard time reckoning with our role in this stratification system we perpetuate is that we lack an outsiders' objective perspective—our peers are also complicit. We have collectively deceived ourselves that this is all okay.

<!-- But we can -->
---

But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

We can start by creating new peer review processes that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness when authors are ready to share their findings. When authors and reviewers cannot agree on how to best improve a submitted work, or cannot agree on when or whether the work is ready to share, we can publish reviewers' dissenting opinions in concert with, or even attached to, the published work. We can publish supporting opinions, too. Such insight into the credibility of research is what the press and the public are actually looking for when they ask whether research has been peer reviewed.

Redesigning peer review without ranking also presents other opportunities to serve science. For human-subjects research, reviewing experimental designs prior to their execution could ensure reviewers are unbiased by the experiments' outcomes and give researchers the opportunity to fix methodological flaws identified by review before conducting their experiments. It's tragic that our peer review regime identifies flaws in experiments only after the resources to conduct the experiment have been exhausted.

Rethinking peer review could also allow us to rethink conferences, for which reviewers rank papers to determine which attendees will want to see, or which they think we *should* want to see, in the limited number of presentation slots. Conferences could instead ask prospective attendees directly which of the scientifically-reviewed candidate presentations they want to see and create a schedule optimized to give us what we *actually* want. They could place host presentations with the most interest in the largest lecture halls; the could give presentations with a small but devoted audience more intimate spaces; they could offer online or recorded presentations of papers with insufficient interest to justify physical space. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

Once it is possible to participate in scientific peer review without ranking, the community of reviewers can include those eager to serve science but unwilling to be complicit in perpetuating stratification.

---

For better or worse, removing ranking from scientific peer review, or even from scientific research, will not bring an end to stratification. The systems and organizations that rely on ranking-focused peer review for stratification will adapt and survive, as they have in the past, so long as there is demand. They are also good at manufacturing demand. In fact, many of the professional societies that host our conferences and publish our journals already have committees dedicated exclusively to stratifying their membership into "[grades](https://awards.acm.org/advanced-member-grades)" that applies to members who who don't publish any research. (While many of us do not see ourselves as advocates for social stratification, we become implicit advocates when conform to the pressure to publicly congratulate peers when they reach to the next strata.)

For aspiring scientists, learning to recognize the toxicity of stratification can hopefully help more survive it. Too much talent is lost because we expect researchers to be great scientists, but to also be "thick-skinned" and perseverant enough to survive in a system that has prioritized ranking over science.

For established scientists who survived and inherited the toxic system from our heros and mentors, we must take responsibility for how our choices perpetuate it. When examined with an outsiders' objective perspective, there is a clear choice of whether to co-opt scientific peer review for ranking: *reject*.

<!-- em — , en – -->
