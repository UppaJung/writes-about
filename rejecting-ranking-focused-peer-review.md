## Let's Peer Review as we Say, Not as we Do

We scientists say we practice peer review to protect the scientific integrity of research: employing objective experts to identify its flaws, evaluate its credibility, and ensure it does not confuse or mislead. Having convinced the press and public that peer review is a signal that research is credible, preventing research from misleading *should* be our paramount goal in conducting peer review.

But in much of science, what we are actually doing when we peer review research is to rank it relative to other research competing for attention and, by extension, to rank our fellow scientists relative to each other. Peer review committees are run by journals and conferences, each of which has its own brand, and which signal brand prestige by being selective in accepting submissions. They use reviewers as gatekeepers to determine which research will be of interest to their audience.<!-- Our ability to accumulate publications with exclusive brands determines our career trajectory and our share of acclaim, attention, and – yes – positions on peer review committees. --> Being selective often means rejecting some submissions of sufficient integrity that lack such subjective qualities such as "novelty", "impact", or that particularly nebulous catch-all (really a *drop-all*) of a "significant contribution". As scientists, our ability to navigate our research through these gates determines our career trajectory in science's stratification system. 

There are myriad ways in which our using peer review to gatekeep what is interesting undermines scientific writing, and is harmful to scientists and to science itself. So when claim peer review protects scientific integrity, supposedly preventing scientists from misleading ourselves and the public, we are mischaracterizing our current practice of peer review in a way that actually misleads ourselves and the public.

Perversely, we make gatekeeping the overriding purpose of peer review even though this function has become almost entirely vestigial to science. Limiting what could be published was necessary in the era when exchanging information was expensive: when research was printed on paper, traveled by mail, and took up space in libraries. In a era with free publication servers we don't need peer reviewers to determine what is worth publishing. To decide what to read, we now have search engines, social sharing, micro-blogging, and myriad other mechanisms to filter through the information that competes for our attention. Yet, we still use peer reviewers to be arbiters of fashion, telling us which research is appealing enough to be modeled on their metaphorical runways and which must be left hanging backstage in science's sweaty dressing rooms.
<!-- Perhaps for the slide version of this talk "Runways are an odd slightly-raised surface for scientists to choose to die on." or "When I ask fellow scientists whether they really want to be arbiters of fashion, I'm often surprised how many will choose a runway as the the hill they're willing to die on." -->

In this manifesto, I'll review some of the way science is harmed when we use peer review as a gatekeeping mechanism and how we can excise gatekeeping to make peer better serve us better serve science, and better serve us all in our roles as authors of research, consumers of research, and peer reviewers.

<!-- ### Part 1. Review -->
#### Gatekeeping research subverts scientific integrity

There are many ways using peer review to rank research, and determine which research is interesting enough to publish, harms scientific writing and causes science to mislead ourselves and others.

**Gatekeeping biases the body of published results**. Submissions that confirm the hopes and beliefs of reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among the many consequences is the potential for false discovery, especially when when peer review committees deem null results insufficiently interesting to publish. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to find specious support the attractive-but-false hypothesis that the intervention is effective—and get published.

**Favoring interesting research causes authors to write less scientifically**. It inspires authors to elide mundane details of experimental designs that might bore reviewers, even if those details would be needed to replicate their experiments. It provokes them to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. It tempts authors to deceive others, and even themselves, into believing that hypothesis tests they conducted on data they found interesting were planned before they had seen the data. It motivates authors to aggrandize their research to look more important. Many publications even explicitly ask that authors go beyond factual reporting of results to speculate about their research's impact and importance.

**Making reviewers gatekeepers changes the audience research is written for**. When authors must craft papers to be accepted by reviewers they must make sacrifices in serving their intended audience. For example, as authors we often add volumes of citations of no value to anyone but our reviewers. We do so to signal to reviewers that we are experts in the field and to reduce the chance of being faulted for failing to cite a reviewer's favorite work. Since reviewers expect papers to have at least as many citations as previously-accepted work, we add even more unnecessary citations when writing on a relatively new topic for which there is little related work. As authors write to conform, not inform, papers have grown to meet reviewers' ever increasing expectations for citation counts, and with cited work receiving less explanation of why its relevant to help the non-expert reader. We also conform by maximizing the amount of information that can fit of a page limit (to maximize the "contribution" needed for acceptance), and using our full page limit, rather than publishing shorter (or longer) papers.

**Gatekeeping delays the spread of knowledge** because comparing competing submissions takes time: authors must often wait to submit their research until a common deadline; reviewers need time to read a cohort of submissions to compare them against each other; committees need time to collectively discuss which papers to accept. These delays can add months to each submission cycle. Research that needs to be submitted three times to find a publication that considers it on brand may take over a year to be accepted and longer to publish.

**Gatekeeping-induced delays disrupt authors' work and degrade our memories**. Time decays our ability to recall how we conducted our experiments, our observations during the experiments, and the tooling we used to process data and document our results. By the time have access to reviewers' feedback identifying what we failed to document, we may have already forgotten details that reviewers noticed we had elided. Coming back up to speed on that which we do remember also takes time and effort. Unless we are lucky enough to have our work accepted on its first submission, we must disrupt our other work to revise our submission to target a different publication, with different reviewers, with different interests, paper formatting rules, and other expectations.

**Standards of review vary by gatekeeper and can be deceptive**. Every journal and conference has different standards and those standards change over time. Even the highest-strata conferences in my field are known to override some reviewer's integrity concerns if others believe the work will draw interest and attention. Lower-strata conferences are known to publish work they know to be flawed to meet financial or attendance goals. At the bottom strata are publications that provide the pretense of peer review while accepting any paper for which authors pay publication fees. Those unfamiliar with a field's varying strata of publications may have no idea whether a work that appears to be peer reviewed was reviewed, and what it was reviewed for.

<!-- --- -->
<!-- **Co-opting peer review to rank research has broader harms.** -->
#### 2. Gatekeeping research is harmful to scientists and science itself

Using peer review as a gatekeeping mechanism in service to stratification also has broader harms.

Conferences and journals that reject a majority of papers will necessarily provide more negative feedback to the community than positive feedback. Peer review committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but the overriding feedback must still justify rejection if a paper is to be rejected. 

The predominance of negative feedback biases who becomes a scientist. The scientific community loses talented aspiring scientists who are uncomfortable promoting their work as important, or who are too "thin-skinned" or "insufficiently perseverant" to discard feedback that conveys their work is uninteresting and unimportant. Those more comfortable promoting the importance of their work or whose sheer self confidence allows them to dismiss negative feedback survive.

Having countless peer review committees of different strata is also burdens us with unnecessary work as reviewers. We are expected to review papers that had already met scientific-integrity requirements when previously rejected. We are asked to write feedback to authors who may be content to send their work unchanged to the next set of reviewers with different subjective expectations of what is important.

Being part of a system that produces toxic feedback for authors is also toxic to us as reviewers. We must work alongside reviewers who abuse the subjectivity and anonymity of peer review. We witness the trauma of those who receive toxic feedback while our obligation to protect the identities of those writing toxic feedback (a condition of participating in the system) and can't help but feel involuntarily complicit in harm, even if we volunteered with the intent of serving science.
<!-- 
If only we could perform our scientific duties without participating in a stratification system toxic to science! -->

Alas, many of us have a hard time even imagining what peer review would be like if our job were not to be arbiters of what should be published.

<!-- --- -->

<!-- ### Part 2. Call to Action -->

#### Conducting *elucidative* scientific peer review

Peer review should have one paramount goal: to prevent research from misleading its audience.

Journalists provide a useful model for working to prevent research from misleading without the ability to keep authors from publishing it. When reporting on new research, the most diligent members of the science press reach out to multiple objective scientific experts, ask those experts to speak to the credibility of research, and quote those experts so that readers know exactly what they said. When experts use language that may be unfamiliar to a broader audience, journalists work to make it understandable to them.

The journalistic model is *elucidative*: it brings concerns and disagreements to the audience's attention rather than using those concerns to gate what the audience should know.

We scientists can emulate this model by writing our reviews to be shared with the audience of the research, to be understandable by that full audience, and to ensure that audience is not mislead.

<!-- We should write reviews to inform the audience of the research we are reviewing to ensure they are not mislead, and not addressed to other reviewers arguing whether the research should or should not be accepted. We might    question of whether the work should be accepted for publication to ensure it doesn't mislead, rather than to  Rather than writing reviews to address whether to allow work to be published, we should write reviews to prevent the work's audience from being mislead.  -->

<!-- , as much as journals and conferences may want us to do so to preserve their prestige and revenue models -->

But as scientific reviewers we can do more when we are concerned that research might mislead, by providing feedback that helps authors improve their work. We can revise our audience-directed feedback to note that the authors' revisions have addressed our concerns concerns, or we can simply remove those concerns from if they no longer apply.

As authors, we may disagree with reviewers' concerns or other parts of a review, and may choose to rebut either through revisions to the work or through separate commentary. Some reviewers may choose to rebut others' reviews, especially if they find a review prejudicial. A process that makes our disagreements apparent, just as reporters do when quoting skeptical experts, is far better than one that sums up reviews into binary "accept" or "reject" decisions.

When authors are ready to publish work as scientifically peer reviewed in the elucidative model, they must make the reviews accessible to their audience. If authors want to publish a revision before the reviews are updated in response, or if reviewers are unwilling or unable to respond, authors must make the reviewed version(s) available along with the published version, illuminating what content was reviewed and what content has since changed.

<!-- Should authors want to publish a revision that is newer than the last version a reviewer addressed, they must make that prior version available as well. This allows their audience to see what content has changed since that reviewer reviewed the work. -->

##### Implementation

There are still many choices in how to institutionalize elucidative peer review. Which institution(s) should run it? How to best choose reviewers of varying expertise, objectivity, and availability for fast turn around? How much to charge authors and pay reviewers? Should authors be anonymous through the entire review process, until a reviewer has submitted their first review, or not at all? Do reviewers need to be anonymous or pseudonymous if they're not in a position to reject papers, or should the audience can evaluate the expertise of the reviewers unless reviewers explicitly opt for anonymity?

The audience will want to know who wrote the peer reviews. Like press sources, reviewers might choose to be anonymous, pseudonymous, or named. When compared to peer review for gatekeeping, reviewers may feel less fearful of retribution and so less likely to expect anonymity.

<!--  when there are no gatekeepersReviewing prior the the experiment also has the benefit of ensuring that reviewers' initial feedback is unbiased by the results. -->

#### 4. How *elucidative* peer review will change science

To make *elucidative* peer review a reality within a field or subfield, we need to create peer review committees dedicated to serving science, that will pair authors work with objective peers. They would act less like committees in that they would not vote to decide which papers are accepted or rejected, but they would likely need to agree on practices such as workload, timelines for initial feedback, and the number and scope of revisions that authors can reasonably expect reviewers to respond to.

<!-- FIXME --  Sum up that we should do this -->


Elucidative peer review can even be started prior to research being conducted, when researchers have merely completed their experimental design and methodology. Starting reviews then would allow flaws to be fixed before incurring the cost to run the experiment. Our current approach identifies experimental flaws only after the work is complete, often after authors have graduated or started other jobs.


<!-- FIXME -- here, talk about benefits beyond toxicity  -->


<!-- FIXME -- good time to point out other uses, such as pre-experiment -->

#### Excising the vestiges of ranking

Practicing peer review for the purpose of ranking is like an anachronistic religious bloodletting ritual, and our journals and conferences are the churches and temples that conduct and perpetuate this rite without scrutiny as to its modern relevance. If authors have access to scientific peer review, there is no reason to conduct peer review for either of these institutions.

Without the pretense of protecting scientific integrity, journals are effectively awards committees that endow prestige onto work within their field or subfield. Those who like awards committees and feel they serve the community can contributing to them. The rest of us should feel no obligation.

Conferences do serve a purpose: bringing researchers together and providing a forum for researchers to present their work. We should be suspicious of the use of peer review committees to curate which research gets presented at conferences, as most review committees are biased in favor of more senior researchers over those with newer ideas and more vulnerable positions. Better to ask prospective attendees directly which of the candidate presentations we are interested in attending and curating a schedule optimized to give us what we want, not want reviewers think we want: allocating the largest spaces to the presentations with the largest audiences and the more intimate spaces to those with small but passionate audiences. Authors of work that have insufficient interest to warrant presentation space can still use the conference to meet people who want to talk to them about their work.

These institutions have the most to lose if we start to conduct *scientific* peer review, putting the needs of those of us who write, consume, and review scientific research above their need to maintain a valuable brand. 

Alas, most of us feel deep loyalty to these institutions and their brands and the validation they give us. We see criticisms of these institutions as attacks on ourselves, our friends, and others in our community who have served as reviewers, program chairs, and editors. Our collective loyalty and complicity with these institutions makes it hard fo us to deprogram ourselves and reckon with the harms they cause.

Some of us avoid reckoning with our complicity by placing all the blame for peer review's toxicity on the most egregious reviewers. This convenient bogeyman somehow produces enough un-constructive ill-informed feedback to reject most of our papers while at the same time being a small minority that none of our friends or respected colleagues are guilty of being associated with.

Others justify perpetuating these institutions in much the same way that homeowners justify protecting their exclusive neighborhoods. We talk about cherishing the traditions and character of our communities. We attribute problems to the inevitable consequences of overcrowding and forces beyond our control.




---




<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->



<!-- Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times. -->

Lastly, many of us excuse our complicity by asserting that there is simply no escape from the system that demands stratification. Journals and conferences need exclusive brands to attract submissions. Hiring and tenure committees need exclusive publications to rank candidates. We simply can't imagine peer review any other way.

<!-- But we can -->
---

---

There will be resistance to science-first peer review. We are accustomed to conducting peer review *for* conferences and journals, when the two stakeholders we really should be working for are authors and their audience. Our collective complicity in using peer review for stratification has left us with few objective peers to question this choice.

Fortunately, there is little in today's peer review system that prevents those who want to request and participate in science-first peer review from doing so. We should establish science-first review committees, volunteer to review for them, and submit to them.

<!-- All we need to start conducting science-first peer reviews is a quorum of scientists ready to put authors, their audience, and reviewers before the journals, conferences, and the professional societies that have historically controlled peer review committees and made peer review serve their purposes. We simply need enough of us willing to conduct reviews, and a mechanism for pairing research to reviewers who are sufficiently objective, sufficiently knowledgeable, and who are able to provide timely feedback.
 -->

And, so long as we participate in peer review that puts stratification over science, we can move forward simply by being honest with the press and the public about what peer review really means when we make that choice. When we can not do as we say, we must at least say what we do, so that objective outsiders can render their opinions. That is, after all, what science should be about.

<!-- Similarly, the more forthcoming we can be with aspiring scientists about the toxicity of social stratification in science, and the role of peer review in that system, the more might survive to help us change it. -->

<!-- And, for better or worse, creating science-first peer review will not bring an end to stratification. The systems and organizations that feed on stratification will continue to find ways to sort us into "[grades](https://awards.acm.org/advanced-member-grades)". There will always be temptations for scientists to put themselves above science. 
 -->
<!-- (While few of us think of ourselves as proponents of social stratification, we become implicit advocates when conforming to the pressure to publicly congratulate peers elevated to the next strata.) We should at least acknowledge and reckon with social stratification in science because our failure to do so has made it harder for aspiring scientists to survive it. -->
 


<!-- em — , en –   …  -->
