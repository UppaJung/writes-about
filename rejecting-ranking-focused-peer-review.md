## Rejecting Ranking-Focused Peer Review

We scientists like to present the purpose of peer review to be protecting scientific integrity: ensuring research is presented clearly, completely, objectively, and accurately. Indeed, preventing research from misleading should be the paramount not only within science, but to the press and public for which peer review is the best available signal of whether to trust research.

Alas, the overriding purpose of peer review, as we actually practice it in most of science, isn't integrity—it's *ranking*.

Peer review is the linchpin of the system that we scientists use to rank the value of research and, by extension, to rank each other. Our peer review committees are run by journals and conferences, each with its own brand and tier of prestige. They chose standards of acceptance and reviewers to signal that prestige.

Peer review for these journals and conferences is the foundation of science's stratification system: we rank the importance of research by where it is published; we rank the importance of scientists by their ability to accumulate publications with prestigious brands; then we use these rankings to decide who is worthy of acclaim, jobs, trust, and attention.

This system requires reviewers to reject papers that merely satisfy scientific-integrity requirements, lest the top strata would be too inclusive. Review committees create exclusivity through requirements such that research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

Our use of peer review for ranking renders scientific integrity secondary and ranking requirements conflict with, and subvert, scientific integrity. And so, when we present peer review only as a process to help prevent scientists from unwittingly misleading ourselves and the public (eliding its primary role as a ranking mechanism) we are, ironically, guilty of misleading ourselves and the public.

<!-- Ranking conflicts with integrity -->
---

One way ranking requirements subvert science is by *biasing what gets published.* Submissions that confirm the hopes and beliefs of peer reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among other things, this increases the likelihood of false discovery, especially when when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to support the attractive-but-false hypothesis that the intervention is effective—and get published.

*Ranking requirements can make writing less scientific.* When writing for ranked peer review, authors will be tempted to elide mundane details of experimental designs that reviewers might find uninteresting, even if those details would be needed to replicate the experiment, in favor of writing that will convince reviewers the work has a significant contribution. Authors will also be tempted to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. They may deceive others, and even themselves, that they had planned to conduct hypothesis tests prior to observing data when in fact they realized such tests should be conducted after observing the data.

*The expectations of ranking-focused reviewers impose unneeded homogeneity*, as reviewers are more likely to reject papers that don't look like previously-accepted papers than those that do. For example, reviewers are likely to reject papers with fewer-than-average citations even if the topic has few works to cite. This can compel authors to add dozens of unnecessary citations to avoid giving reviewers the impression they are unfamiliar with related work in the field. It's no wonder papers have evolved to contain ever longer chains of citation numbers, most of which are authors add solely to reduce our chance of rejection. If authors had the freedom to write papers to inform, rather than conform, we would use that space to provide more insight into the smaller set of truly relevant prior work.

Similarly, reviewers expect papers to include discussions that go beyond presenting results and speculate about broader implications of those results. Authors who are reluctant to add such subjectivity to scientific writing are compelled to conform to this tradition.

*Ranking delays the spread of knowledge* because ranking-focused peer review is often structured as an admissions process. Admissions processes force authors to submit research at specific deadlines and must give reviewers time to review enough submissions to rank them relative to each other, then to compare papers and discuss acceptance cut-offs. Each submission takes months, and papers must be re-submitted until it reaches a publication that considers the research worthy of its brand. A paper that needs to be submitted three times may take a year to be accepted and longer to publish.

*Delays and re-submissions are disruptive to conducting science.* After months of waiting for reviews, authors are are expected to come back to speed on what we have written, revise per the subjective concerns of reviewers, and (unless we were lucky enough to be accepted in the first round) re-submit to a different set of reviewers with different (often conflicting) expectations, paper formatting requirements, and other rules and expectations.

---

Ranking-focused peer review also leads to excessively negative feedback. It's unavoidable because reviewers feel obligated to provide feedback that justifies the outcome to their fellow committee members, to themselves, and to the authors. When rejecting a submission that feedback will inherently be mostly negative, even if committees try to encourage that the negative feedback be constructive. A system that must reject the majority of papers will provide much more negative feedback to the community than positive feedback, which is harmful to the community's mental health and particularly toxic to the most junior and least powerful members.

The systems we've evolved to support ranking-focused peer review are also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific-integrity requirements when previously rejected. We write feedback to authors who may be content to send their work unchanged to the next set of reviewers.

We feel obligated to participate in peer review because of its importance for protecting scientific integrity, but it is nearly impossible to do without becoming complicit in the harm to authors. We are obligated to work with reviewers who abuse the subjectivity and anonymity of peer review, and obligated to protect their identities, while witnessing the trauma that negative feedback inflicts on others, especially promising aspiring scientists who abandon careers in research. Being obligated to participate in a toxic system is toxic.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to have survived ranking-focused peer review justify perpetuating this system like homeowners protecting their exclusive neighborhoods and cities. We talk about protecting the traditions and character of our communities and attribute problems to the inevitable consequences of overcrowding and forces beyond our control. We ask why we should have to experience change rather than why the current system is justified. We respond to criticisms of the system as attacks on ourselves, our friends, and and others in our community who have served as reviewers, program chairs, and editors.

We blame peer review's toxicity exclusively on the most egregious reviewers—a convenient *other* that is somehow among us despite not including any of our friends or respected colleagues. The most toxic reviewers make great scapegoats, but they are merely the most visible symptoms of the very cancer we ourselves are feeding. We collectively receive far more un-constructive ill-informed feedback than could be produced by this bogeyman.

We also deceive ourselves that others want and need our expertise to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable. A system for identifying that which is new and interesting should not be biased to favor those most set in their ways.

Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does. Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times.

Lastly, we may excuse our complicity by believing that there is simply no escape from using peer review for ranking and stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.

Ironically, one reason we have a hard time reckoning with our role in this stratification system we perpetuate is that we lack an outsiders' objective perspective—our peers are also complicit. We have collectively deceived ourselves that this is all okay.

<!-- But we can -->
---

But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

We can start by creating new peer review regimes that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness as a service to authors. When authors and reviewers cannot agree on how to best improve a submitted work, or cannot agree on when or whether the work is ready to share, we can publish reviewers' dissenting opinions in concert with, or even attached to, the published work. We can publish supporting opinions, too. Such insight into the credibility of research is what the press and the public are actually looking for when they ask whether research has been peer reviewed. We should live up to this ideal.

Redesigning peer review without ranking also presents other opportunities to serve science. For human-subjects research, reviewing experimental designs prior to their execution could ensure reviewers are unbiased by the experiments' outcomes and give researchers the opportunity to fix methodological flaws identified by review before conducting their experiments. It's regrettably wasteful that our current peer review regime identifies flaws in experiments only after the resources to conduct the experiment have been exhausted, and often after some of the experts who conducted the research have graduated or moved onto new jobs.

Rethinking peer review could also allow us to rethink how we distribute the limited presentation space and time slots conferences. Reviewers currently rank papers to curate which they think attendees will want to see, or which they think we *should* want to see. Conferences could instead ask prospective attendees directly which of the scientifically-reviewed candidate presentations they want to see and create a schedule optimized to give us what we *actually* want. Conferences could place host presentations with the most interest in the largest lecture halls; they could give presentations with a small but devoted audience more intimate spaces; they could offer online or recorded presentations of papers with insufficient interest to justify physical space. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

Once it is possible to participate in scientific peer review without ranking, the community of reviewers can include those eager to serve science but unwilling to be complicit in perpetuating stratification. Those who still want to rank papers for prestigious publications, or to give out awards, will still be able to.

---

And, for better or worse, removing ranking from scientific peer review, or even from scientific research, will not bring an end to stratification. The systems and organizations that rely on ranking-focused peer review for stratification will adapt and survive, as they have in the past, so long as there is demand. They are also good at manufacturing demand. In fact, many of the professional societies that host our conferences and publish our journals already have committees dedicated exclusively to stratifying their membership into "[grades](https://awards.acm.org/advanced-member-grades)" that applies to members who who don't publish any research. (While many of us would deny promoting social stratification, we become implicit advocates when conforming to the pressure to publicly congratulate peers elevated to the next strata.)

Still, we need to acknowledge and reckon with social stratification in science because our failure to do so has made it harder for aspiring scientists to survive it. Too much talent is lost because we expect researchers to be great scientists, but to also be "thick-skinned" and perseverant enough to survive in a system that has prioritized ranking over science.

For established scientists, who inherited a system that co-opts peer review for ranking, who have survived it, and who have been conditioned to accept it, we must at least be honest with outsiders about how we actually practice peer review in science. We should also not be surprised when those objective outsiders reach a different conclusion about the choices we make to perpetuate it: *strong reject*.

<!-- em — , en – -->
