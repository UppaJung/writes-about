## What if we Conducted Peer Review to Actually do what we Say it Does?

We scientists often depict the goal of peer review as protecting scientific integrity: ensuring research is presented clearly, completely, objectively, and accurately. The press and public use peer review to determine whether to trust research – as we scientists do when examining works outside our domain of expertise – and so preventing research from misleading *should* be paramount.

But, in much of science, protecting scientific integrity is not the predominant goal of peer review—in practice we use peer review to rank the importance of research and, by extension, to rank each other. We publish our research through journals and conferences, each of which with its own brand, and each of which signals its strata of prestige by being selective in accepting submissions. In this stratification system, our ability to accumulate publications with exclusive brands determines our career trajectory and our share of acclaim, attention, and – yes – positions on prestigious peer review committees.

Peer review committees achieve exclusivity by rejecting many submissions that merely satisfy scientific-integrity requirements. To do so, they add requirements tailored to their stratification goals, such that research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*. 

There are many ways that this co-option of peer review for stratification conflicts with, and subverts, scientific integrity. And so, when we claim to conduct peer review to prevent scientists from unwittingly misleading ourselves and the public, and elide how we use it for stratification, we have already mislead ourselves and the public through this omission.

<!-- Ranking conflicts with integrity -->
---

One way that co-opting peer review for stratification can cause science to mislead is by *biasing what gets published*. Submissions that confirm the hopes and beliefs of reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among the many consequences is the potential for false discovery, especially when when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to find specious support the attractive-but-false hypothesis that the intervention is effective—and get published.

*These expectations undermine how we document our research.* When reviewers demand that papers are interesting and that writing flows well, authors will be tempted to elide mundane details of experimental designs that might bore reviewers, even if those details would be needed to replicate their experiments. Authors will also be tempted to highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. And they may be tempted to deceive others, and even themselves, to believe that they had planned to conduct hypothesis tests prior to observing data when in fact their decision to conduct a test was influenced by differences they observed in the data.

*These expectations makes papers less readable*, as reviewers expect papers to look like previously-accepted papers, whereas great scientific writing can take many forms that don't fit this mold. For example, reviewers are likely to reject papers with fewer-than-average citations even if the topic has few works to cite. This expectation can compel authors to add dozens of unnecessary citations, lest reviewers suspect they are insufficiently knowledgeable about the field. It's no wonder papers have evolved to contain ever longer chains of citation numbers, most of which have no value but to defend authors from rejection. If authors had the freedom to write papers to inform, rather than conform, we could use that space to provide more insight into the smaller set of truly relevant prior work.

*These expectations can make writing more subjective*, such as when authors are compelled to add discussions that go beyond factual reporting of results to speculate about broader implications and impact of those results.

*Our stratification system delays the spread of knowledge* because peer review designed for stratification is an often structured as admissions process: authors may only submit research at specific deadlines and must give reviewers time to review enough submissions to rank them relative to each other, then to compare papers and discuss acceptance cut-offs. Each submission takes months, and work must be re-submitted until accepted. Research that needs to be submitted three times to find a publication that considers it on brand may take a year to be accepted and longer to publish.

*Our stratification system disrupts scientists work by delaying feedback and requiring resubmissions.* After months of waiting for reviews, authors are are expected to come back to speed on what we have written and revise it to address the subjective concerns of reviewers. Unless we are lucky enough to be accepted in the first round, we must re-submit to a different set of reviewers with different (often conflicting) expectations, paper formatting rules, and other customs.

*Our stratification system lowers the scientific-integrity standards of "peer review"*, because while high-strata publications have too many papers that meet scientific-integrity standards, lower-strata conferences and journals may need to publish sub-standard papers to meet financial or attendance goals. At the bottom strata, papers may only appear to be peer reviewed, and be published without feedback so long as authors pay publication fees or conference-registration fees. By using "peer reviewed" synonymously with "accepted for publication" without any proof that actual review has taken place, we render the term valueless.

---

By co-opting peer review for stratification we also subvert science by biasing who becomes a scientist. Many aspiring scientists fail to survive the mostly negative feedback they receive from the community they are trying to join, even if they are more talented than some survivors who were more self confident or better able to promote the importance of their work. This talent will be discarded as too "thin-skinned" or "insufficiently perseverant".

And peer review co-opted for stratification cannot help but have mostly negative feedback, as most papers will be rejected, and most reviews will recommend rejection. Reviewers are obligated provide feedback to justify their recommendations to themselves, to their fellow committee members, and to the authors. Committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but if the overriding feedback must still justify rejection. Rejecting a majority of papers will necessarily provide more negative feedback to the community than positive feedback, which is harmful to the community's mental health and especially to those most junior and least powerful.

Peer review regimes built for stratification are also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific-integrity requirements when previously rejected. We write feedback to authors who may be content to send their work unchanged to the next set of reviewers. Experiments with known flaws, which should be published with disclosures of those flaws, bounce around until reaching reviewers willing to do so.

We reviewers feel obligated to participate in peer review because we been conditioned to believe in its importance for protecting scientific integrity, but doing so requires us to work alongside reviewers who abuse the subjectivity and anonymity of peer review. We are obligated to protect their identities, but powerless to prevent the trauma their negative feedback inflicts on others. When peer review is primarily used for stratification it is impossible to contribute without becoming complicit.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to have survived peer review co-opted for stratification justify perpetuating this system like homeowners protecting their exclusive neighborhoods and cities. We talk about cherishing the traditions and character of our communities. We attribute problems to the inevitable consequences of overcrowding and forces beyond our control. We respond to criticisms of the system as attacks on ourselves, our friends, and others in our community who have served as reviewers, program chairs, and editors. We ask why we should have suffer changes to the system as opposed to asking how we can justify perpetuating a system that makes others suffer.

Many of us blame peer review's toxicity exclusively on the most egregious reviewers. This convenient bogeyman somehow produces enough un-constructive ill-informed feedback to reject most of our papers while at the same time being a small minority that none of our friends or respected colleagues are guilty of being associated with.

Many of us also convince ourselves that others want and need our expertise to "curate" research to separate the *important* from the merely *accurate*. Yet, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and mental health are most vulnerable. Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does.
<!-- Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times. -->

Lastly, many of us excuse our complicity by asserting that there is simply no escape from co-opting peer review for stratification, as hiring and tenure committees rely on it to rank people.

<!-- But we can -->
---

But, we *can* and *should* isolate peer review for scientific integrity from that used for stratification.

We can start by creating new science-first peer review regimes that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness as a service to authors.

While it would be ideal for reviewers to provide only feedback that authors found valuable, and for authors to make all the changes requested by reviewers, science-first peer review does not require such agreement. Rather, reviewers who are unsatisfied should be empowered to publish dissenting opinions in concert with, or even attached to, the published work. Combined with supporting opinions from more positive reviewers, such insight into the discoveries and limitations of research are what the press and the public are actually looking for when they ask whether research has been peer reviewed. Better that we live up to their expectations than lower those expectations.

Science-first peer review could include the evaluation of experimental designs *prior* to their execution. Currently, the only peer review typically performed prior to an experiment's execution is ethical review, and typically only for human subjects experiments. Prior review would ensure that reviewers' feedback is unbiased by an experiments' outcomes. Prior review would also give researchers the opportunity to fix methodological flaws before they start conducting their experiments. Our current peer review regime is regrettably wasteful in identifying flaws in experiments only after the resources to conduct the experiment have been exhausted, and often after some of the experts who conducted the research have graduated or moved onto new jobs.

Science-first peer review could also allow us to rethink how we distribute the limited presentation space and time slots conferences. Reviewers currently try to curate a set of papers they think attendees will want to see, or that they think we *should* want to see. Conferences could instead ask prospective attendees directly which of the candidate presentations we want to see and curate a schedule optimized to give us what we *actually* want. Conferences could host presentations with the most interest in the largest lecture halls; they could give presentations with a small but devoted audience more intimate spaces; they could offer online or recorded presentations of papers with insufficient interest to justify physical space. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

---

One challenge in transitioning to science-first peer review is that our collective complicity in co-opting peer review for stratification has left us no objective peers to tell us to reject this co-option. We should.

Until we are able to do so, we should at least be honest in depicting the dual purposes of peer review to the public and sharing our concerns about its efficacy in protecting integrity. Similarly, the more forthcoming we can be with aspiring scientists about the toxicity of social stratification in science, and the role of peer review in that system, the more might survive to help us change it.

<!-- And, for better or worse, creating science-first peer review will not bring an end to stratification. The systems and organizations that feed on stratification will continue to find ways to sort us into "[grades](https://awards.acm.org/advanced-member-grades)". There will always be temptations for scientists to put themselves above science. 
 -->
<!-- (While few of us think of ourselves as proponents of social stratification, we become implicit advocates when conforming to the pressure to publicly congratulate peers elevated to the next strata.) We should at least acknowledge and reckon with social stratification in science because our failure to do so has made it harder for aspiring scientists to survive it. -->
 


<!-- em — , en –   …  -->
