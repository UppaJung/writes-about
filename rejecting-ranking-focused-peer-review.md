## Peer Review as we Say, Not as we Do

We scientists say we conduct peer review to protect scientific integrity: ensuring research is presented clearly, completely, objectively, and accurately. We have taught the press and public to ask whether research is peer reviewed when gauging how much to trust it, and so preventing research from misleading *should* be our paramount goal. 

But in much of science, what we are actually doing when we peer review research is to rank its importance relative to other research, by extension, to rank our fellow scientists. We conduct peer review through journals and conferences, each of which with its own brand, and each of which signals its strata of brand prestige by being selective in accepting submissions. In this stratification system, our ability to accumulate publications with exclusive brands determines our career trajectory and our share of acclaim, attention, and – yes – positions on prestigious peer review committees with prestigious brands.

Peer review committees for prestigious publications achieve exclusivity by rejecting many submissions that merely satisfy scientific-integrity requirements. To determine which are important enough to publish, they add requirements such that research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*. 

There are many ways that ranking work on its subjective *importance* subverts the goal of protecting scientific integrity. And so, when we purport to conduct peer review to prevent the scientists who conduct research from unwittingly misleading ourselves and the public, and elide that we are actually doing is ranking their research using integrity as just one dimension of importance, we have ironically already mislead ourselves and the public.

<!-- Ranking conflicts with integrity -->
---

*Ranking importance misleads by biasing what gets published*. Submissions that confirm the hopes and beliefs of reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Among the many consequences is the potential for false discovery, especially when when peer review committees deem null results insufficiently interesting to publish. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to find specious support the attractive-but-false hypothesis that the intervention is effective—and get published.

*Ranking importance undermines how we document our research.* When reviewers demand that papers be interesting and that writing flow well, authors elide mundane details of experimental designs that might bore reviewers, even if those details would be needed to replicate their experiments. Authors highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting. Some authors deceive others, and even themselves, to believe that they had planned to conduct hypothesis tests prior to observing data when in fact their decision to conduct a test was influenced by differences they observed in the data.

*Ranking importance make papers less readable*, as reviewers expect papers to look like previously-accepted papers, whereas great scientific writing can take many forms that don't fit this mold. For example, reviewers often reject papers with fewer-than-average citations even if the topic has few works to cite. So authors often add dozens of unnecessary citations to ensure their count is above average, signal their expertise in the field, and reduce the chance that a reviewer will reject their work having failed to cite a reviewer's favorite work. It's no wonder papers have evolved to contain increasingly longer chains of citation numbers, with decreasing amounts context to assist the non-experts who actually benefit from citations to related work. If authors had the freedom to write papers to inform, rather than conform, we could use that space to provide more insight into the smaller set of truly relevant prior work.

*Ranking importance encourages subjective writing*, because it encourages authors to aggrandize their research to look more important. Many publications even demand that authors go beyond factual reporting of results to speculate about their research's impact and importance.

*Ranking importance delays the spread of knowledge*. Authors must often wait until specific deadlines to submit research, then give reviewers time to review enough submissions to rank them relative to each other, and time for reviewers to collectively discuss papers in comparison to each other. Each submission typically takes months, and work must be re-submitted until accepted. Research that needs to be submitted three times to find a publication that considers it on brand may take over a year to be accepted and longer to publish.

*The delays for ranking also disrupts authors' work and degrades our memories*. Feedback arrives months after we last worked on the paper and time impedes our memories of what we wrote, how we conducted our experiments, our findings, and the tooling we used to process our data. Coming back up to speed takes time. By the time have access to reviewers' feedback identifying what we failed to document, we may have already forgotten the missing details. Unless we are lucky enough to be accepted in the first round, we must re-submit to a different set of reviewers with different (often conflicting) expectations, paper formatting rules, and other customs. As more months pass, our memories will degrade further. Time that we could use to further science is instead used to make our work conform to another committees expectations for what is important.

*Because we use "peer reviewed" synonymously with "accepted for publication" it carries no signal of scientific integrity*. While high-strata publications have too many papers that meet scientific-integrity standards, lower-strata conferences and journals may need to publish sub-standard papers to meet financial or attendance goals. At the bottom strata, submissions may only appear to be peer reviewed, receive no feedback from reviewers, and may be published regardless of content so long as authors pay publication fees.

---

Peer reviewing to rank subjective importance also subverts science by biasing who becomes a scientist. Many aspiring scientists fail to survive the mostly negative feedback they receive from the community they are trying to join, even if they are more talented than some survivors who were more self confident or better able to promote the importance of their work. This talent will be discarded as too "thin-skinned" or "insufficiently perseverant".

And peer review co-opted for stratification cannot help but have mostly negative feedback. Peer review committees may try to encourage reviewers to list some positives for every paper and to be constructive in presenting negative feedback, but the overriding feedback must still justify rejection if a paper is to be rejected. Any system that rejects a majority of papers will necessarily provide more negative feedback to the community than positive feedback. A system that provides mostly negative feedback is harmful to the mental health of those who rely upon it, especially to those most junior who have the most to lose and the least control over how the system works.

The stratification system built on peer reviewing for importance is also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific-integrity requirements when previously rejected. We write feedback to authors who may be content to send their work unchanged to the next set of reviewers with different subjective expectations of what is important. Experiments with known flaws, which should be published with disclosures of those flaws, are submitted over and over, to be reviewed over and over, until reaching reviewers who think it's important to publish research with disclosed flaws.

We reviewers feel obligated to participate in peer review that ranks importance because we been conditioned to believe that any form of peer review benefits science by protecting scientific integrity. yet, participating on such peer review committees requires us to work alongside reviewers who abuse the subjectivity and anonymity of peer review. We witness the trauma of those who receive toxic feedback while our obligation to protect the identities of those writing toxic feedback (a condition of participating in the system) makes us involuntarily complicit.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to have survived peer review co-opted for stratification justify perpetuating this system like homeowners protecting their exclusive neighborhoods and cities. We talk about cherishing the traditions and character of our communities. We attribute problems to the inevitable consequences of overcrowding and forces beyond our control. We respond to criticisms of the system as attacks on ourselves, our friends, and others in our community who have served as reviewers, program chairs, and editors. We ask why we should have suffer changes to the system as opposed to asking how we can justify perpetuating a system that makes others suffer.

Many of us blame peer review's toxicity exclusively on the most egregious reviewers. This convenient bogeyman somehow produces enough un-constructive ill-informed feedback to reject most of our papers while at the same time being a small minority that none of our friends or respected colleagues are guilty of being associated with.

Many of us also convince ourselves that others want and need our expertise to "curate" research to separate the *important* from the merely *accurate*. Yet, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, who are also the most vulnerable. Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does.
<!-- Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times. -->

Lastly, many of us excuse our complicity by asserting that there is simply no escape from the system that demands stratification. Journals and conferences need exclusive brands to attract submissions. Hiring and tenure committees need exclusive publications to rank candidates. We simply can't imagine peer review any other way.

<!-- But we can -->
---

But, we *can* and *should* put science first by conducting peer review for the reasons we say we do:

1. to help *authors* improve the objectivity, accuracy, understandability, and completeness of their work, and
2. to help *their audience* evaluate the credibility of the work from an objective perspective, and avoid being mislead.

Science-first peer review should not rank research, or even decide what should and shouldn't be published. Reviewers should act only to help authors improve the work and to provide their audience with an objective evaluation of its credibility. Full stop. This is what the press and public are actually need when they ask whether research has been peer reviewed.

To publish research as "peer reviewed", authors need only give their audience access to reviewers' feedback, ideally by linking to it so that reviews can be updated. If the most recent revision of the work is different from the version a reviewer last provided feedback on, authors must make the version that reviewer provided feedback on available so that readers can see what content has changed. Modified content must not be falsely presented as having been peer reviewed.

When authors revise their work they can request that reviewers revise their reviews. Reviewers can submit new revised feedback at any time, such as after reading others' reviews or reading rebuttals or other comments from authors.

Since science-first peer review isn't used to decide whether research should be accepted or rejected, the most adversarial thing reviewers can do is write unfairly negative feedback. The authors and other reviewers can rebut this feedback through the review process, and can anyone else via public forums such as blogging, social media, or follow-up publications.

Because science-first peer review fosters a cooperative relationship between reviewers and authors, reviewers will have less reason to remain anonymous as they typically do when decide which work should or should not be accepted for publication. Some reviewers still might still prefer to critique others' work anonymously, but others might choose to write reviews under their own names or write all their reviews under a single pseudonym. Such transparency would reassure the audience as to the expertise and independence of the reviewers selected to provide peer feedback.

<!-- We should be protectors of truth, not gatekeepers of speech. -->

In addition to reviewing completed research, science-first peer review could help researchers evaluate their experiments designs *prior* to conducting their experiments. Currently, only ethical review takes places prior to running an experiment, and only when the experiment involves human subjects or poses other ethical risks. Reviewing experimental designs before execution would ensure that reviewers' initial feedback is unbiased by an experiments' outcomes, and would give researchers the opportunity to fix methodological flaws before they start conducting their experiments. Our current peer review regime is regrettably wasteful in identifying flaws in experiments only after the resources to conduct the experiment have been exhausted, and often after some of the experts who conducted the research have graduated or moved onto new jobs.

Adopting science-first peer review would also give us the opportunity to rethink how we distribute the limited presentation space and time slots conferences. Reviewers currently try to curate a set of papers they think attendees will want to see, or that they think we *should* want to see. Conferences could instead ask prospective attendees directly which of the candidate presentations we want to see and curate a schedule optimized to give us what we *actually* want. Conferences could host presentations with the most interest in the largest lecture halls; they could give presentations with a small but devoted audience more intimate spaces; they could offer online or recorded presentations of papers with insufficient interest to justify physical space. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

---

There will be resistance to science-first peer review. We are accustomed to conducting peer review *for* conferences and journals, when the two stakeholders we really should be working for are authors and their audience. Our collective complicity in using peer review for stratification has left us with few objective peers to question this choice.

Fortunately, there is little in today's peer review system that prevents those who want to request and participate in science-first peer review from doing so. We should establish science-first review committees, volunteer to review for them, and submit to them.

<!-- All we need to start conducting science-first peer reviews is a quorum of scientists ready to put authors, their audience, and reviewers before the journals, conferences, and the professional societies that have historically controlled peer review committees and made peer review serve their purposes. We simply need enough of us willing to conduct reviews, and a mechanism for pairing research to reviewers who are sufficiently objective, sufficiently knowledgeable, and who are able to provide timely feedback.
 -->

And, so long as we participate in peer review that puts stratification over science, we can move forward simply by being honest with the press and the public about what peer review really means when we make that choice. When we can not do as we say, we must at least say what we do, so that objective outsiders can render their opinions. That is, after all, what science should be about.

<!-- Similarly, the more forthcoming we can be with aspiring scientists about the toxicity of social stratification in science, and the role of peer review in that system, the more might survive to help us change it. -->

<!-- And, for better or worse, creating science-first peer review will not bring an end to stratification. The systems and organizations that feed on stratification will continue to find ways to sort us into "[grades](https://awards.acm.org/advanced-member-grades)". There will always be temptations for scientists to put themselves above science. 
 -->
<!-- (While few of us think of ourselves as proponents of social stratification, we become implicit advocates when conforming to the pressure to publicly congratulate peers elevated to the next strata.) We should at least acknowledge and reckon with social stratification in science because our failure to do so has made it harder for aspiring scientists to survive it. -->
 


<!-- em — , en –   …  -->
