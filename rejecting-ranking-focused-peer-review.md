## Rejecting Ranking-Focused Peer Review

We scientists like to propagate the convenient myth that the primary purpose of peer review is one of scientific integrity: to ensure research informs without misleading. Indeed, ensuring presented clearly, completely, objectively, and accurately *should* be paramount to scientists.

Alas, the overriding purpose of peer review, as we actually practice it in most of science, is *ranking*.

Peer review is the linchpin of the system that we scientists use to rank the value of research and, by extension, to rank each other. Our peer review committees are run by journals and conferences, each with its own brand and tier of prestige. They chose standards of acceptance and reviewers to signal that prestige.

Peer review for these journals and conferences is the foundation of science's stratification system: we rank the importance of research by where it is published; we rank the importance of scientists by their ability to accumulate publications with prestigious brands; then we use these rankings to decide who is worthy of acclaim, jobs, trust, and attention.

This system requires reviewers to reject papers that merely satisfy scientific-integrity requirements, lest the top strata would be too inclusive. Review committees create exclusivity through requirements such that research be *novel*, *interesting*, *impactful*, or that it meets the particularly nebulous catch-all (or *drop-all* for rejecting any submission) of having a *significant contribution*.

<!-- Ranking conflicts with integrity -->
---

Not only does ranking render scientific integrity secondary, *ranking requirements* conflict with, and subvert, scientific integrity.

*Ranking requirements bias what gets published.* Submissions that confirm the hopes and beliefs of peer reviewers are more likely to be accepted than those that defy reviewers' expectations or dash their hopes. Consider, for example, the consequences when peer review committees deem null results unpublishable. If twenty research teams test an attractive but ineffective intervention, we can expect 19 teams to to find no significant difference ($p>0.05$) and fail to publish. We can expect one of twenty to support the attractive-but-false hypothesis that the intervention is effective and get published.

*Ranking requirements can make writing less scientific.* When writing for ranked peer review, authors elide mundane details of experimental designs that reviewers might find uninteresting, even if those details would be needed to replicate the experiment. Authors highlight experiments or tests that yielded a statistically significant or novel result and dedicate less space to those that reviewers will find less interesting.

*The expectations of rank-focused reviewers impose unneeded homogeneity*, as they are more likely to reject papers that don't look like previously-accepted papers. For example, they are likely to reject papers with fewer-than-average citations even if the topic has few works to cite. This can compels authors to add dozens of unnecessary citations to avoid giving reviewers the impression they are unfamiliar with related work in the field. It's no wonder papers have evolved to contain ever longer chains of citation numbers, most of which are authors add solely to reduce our chance of rejection. If authors had the freedom to write papers to inform, rather than conform, we would use that space to provide more insight into the smaller set of truly relevant prior work.

Similarly, reviewers expect papers to include discussions that speculate about implications. Those who are reluctant to add such  subjectivity are compelled by to do so by tradition.

*The process of submitting and re-submitting work to new reviewers takes time away from conducting science, and the timely publication of results.* Writing and re-writing papers to conform to the expectations of rank-based peer review takes time. Each rejection adds time and adds time between when research is complete and when others can learn about it. Science is best served if review is fast and work only needs to be revised to meet one set of reviewers.

---

Peer review is systemically toxic, and will be, so long as scientists allow it to be co-opted for ranking. In rejecting papers, reviewers feel obligated to provide feedback that justifies the outcome to their fellow committee members, to themselves, and to the authors—feedback that must inevitably focus on shortcomings to justify rejection. A system that must reject the majority of papers, and provide feedback focused on justifying rejection, is harmful to the mental health of the community receiving far more negative feedback than positive feedback.

Ranking requirements also toxic to reviewers. We are burdened by the work of reviewing papers that had already met scientific integrity requirements when previously rejected. Since our rank-focused peer review systems encourage authors to re-submit work rejected by one peer review committee to other peer review committees, authors may discard constructive feedback we worked to write as we won't be the ones evaluating it. We feel obligated to participate in peer review because of its importance for protecting scientific integrity, but it is nearly impossible to do so without becoming complicit in the toxicity of the social stratification system built upon it. We are obligated to work with reviewers who abuse the subjectivity and anonymity of peer review, and obligated to protect their identities, while witnessing the damage they inflict that causes aspiring scientists to abandon careers in science.

<!-- The lies we tell ourselves -->
<!-- #### Facing the truth about ranking -->
---

<!-- Metaphor of gated community and of NIMBY construction limits -->
Many of us privileged enough to survive ranking-focused peer review explain our reasons for perpetuating this system using justifications eerily similar to the ones homeowners in exclusive neighborhoods use to fight opening them up. We talk about overcrowding and the loss we would feel if the traditions and character of our communities changed. We ignore the question of whether we are perpetuating a harmful system, and instead ask why we should have to give up that which we grew up with. We respond to criticisms of the system as attacks on ourselves, our friends, and our community (fellow reviewers, program chairs, and editors).

Ironically, the reason we are blind to the harms of ranking-focused peer review systems we are perpetuating is because we've all be part of the system so long that we lack an outsiders' objective perspective to see its flaws—we have no objective peers as we all participate and our complicit in this system. And so, we've managed to collectively deceive ourselves into believing this is all okay.

We blame peer review's toxicity lies exclusively on the most egregious reviewers—a convenient *other* that is somehow among us despite not including any of our friends or close colleagues. The truth is that we collectively reject far more work than a few toxic reviewers could achieve. The most toxic reviewers make great scapegoats, but they are merely the most visible symptoms of the very cancer we ourselves are feeding.

We also deceive ourselves by believing that, as reviewers, we provide value by helping to "curate" research to separate the *important* from the merely *accurate*. In fact, review committees are biased by design to be more senior than the average researcher, and so our collective opinions of what ideas and developments are important cannot help but be biased against the most junior researchers, whose careers and and mental health are most vulnerable. No self-respecting social scientist would intentionally design a system biased in this way.

Using peer review for curation may have had value before papers could be shared for free online and before the invention of myriad tools to filter the resulting onslaught of information. It no longer does. Yet even those of us in Computer Science, whose technical contributions frequently disrupt other industries, are strangely averse to changing with the times.

Lastly, we may excuse our complicity by believing that there is simply no escape from using peer review for ranking and stratification, as the universities and research labs that employ most of us rely on rankings for hiring and promotions.


But, we *can* and *should* isolate peer review for scientific integrity from that used for ranking.

<!-- But we can -->
---


We can start by creating new peer review processes that eschew the question of whether research is worth publishing, acting only to assist in increasing objectivity, accuracy, understandability, and completeness when it is published. Wen authors and one or more reviewers cannot agree on how to best improve a submitted work, or cannot agree on when or whether the work is ready to publish, we can publish reviewers' dissenting opinions in concert with, or even attached to, the published work. Such insight into the credibility of research is what the press and the public are actually looking for when they ask whether research has been peer reviewed.

Redesigning peer review to make science paramount also presents other opportunities. For human-subjects research, reviewing experimental designs prior to their execution could not only ensure reviewers are unbiased by the experiments' outcomes, but could allow methodological flaws identified by reviewers to be fixed before experiments are conducted.

Rethinking peer review could also allow us to rethink conferences, which currently rank papers to determine which are worthy of presentation slots. We could instead ask prospective attendees which of the scientifically-reviewed candidate presentations they would want to see and create a schedule optimized to give attendees what we want, rather than what reviewers guess we want, or what reviewers think we should want. Presentations with the most interest could be hosted in the largest lecture halls; presentations with a small but devoted audience could appear in more intimate spaces; papers with insufficient interest could be offered online or recorded. Some research, such as replication studies, might not even need to be presented, and the conference might serve as an opportunity for the the authors to make themselves available in person for readers' questions.

And once we have removed ranking from scientific peer review, the community of reviewers can grow to include those of us who want to serve science but are unwilling to do so if it makes us complicit in the toxicity of ranking-focused peer review.

---

For better or worse, removing ranking from scientific peer review, or even from scientific research, will not bring an end to ranking. The systems and organizations that rely on ranking-focused peer review for stratification will adapt and survive, as they have in the past, so long as there is demand for them. In fact, many of the professional societies that host our conferences and publish our journals already have committees dedicated exclusively to stratifying their membership into "[grades](https://awards.acm.org/advanced-member-grades)" that applies to members who who don't publish any research. (While many of us wouldn't see ourselves as promoting stratification systems, we do so whenever we congratulate colleagues promoted to new grades, and there is a strong social pressure to conform to the norm of publicly congratulating our peers.)

For aspiring scientists, learning to recognize the toxicity of stratification is key to surviving it and, hopefully, surviving to be part of the change.

Until we change how we conduct peer review, each successive generation fo scientists will exclude amazing talent that failed to be "thick-skinned" and perseverant enough to survive in a system that has prioritized ranking over scientific integrity. Each generation will inflict harm on the next for no better reason that that "we've always done it this way" and we don't know how to change. The only way out is to consider ranking-focused peer review from the perspective of an objective outsider, from which there is only one reasonable conclusion: *reject*.

<!-- em — , en – -->
